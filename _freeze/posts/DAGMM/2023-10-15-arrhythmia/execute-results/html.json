{
  "hash": "831247fe92f57bc7b70a5046f4292127",
  "result": {
    "markdown": "---\ntitle: \"**[DAGMM]** DAGMM: for arrhythmia data set\"\nauthor: \"kione kim\"\ndate: \"10/19/2023\"\nbibliography: dagmm.bib\n---\n\n## Deep Autoencoding Gaussian Mixture Model for Arrhythmia dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n### imports\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport sys\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n### data 파일\nfile_path = 'C:\\\\Users\\\\UOS\\\\Desktop\\\\연구\\\\5. 데이터\\\\data\\\\arrhythmia\\\\arrhythmia.data'\n\ndf = pd.read_csv(file_path, header=None)\ndf = df.replace('?', 0)\ndf = df.astype('float64')\n\ndata_array = df.values\ndata_array = torch.autograd.Variable(torch.from_numpy(data_array).float())\ndata_array.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntorch.Size([452, 280])\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nparser = argparse.ArgumentParser(description='parser for argparse test')\n\nparser.add_argument('--input_dim', type=int, default=data_array.shape[-1])\nparser.add_argument('--enc_hidden_dim', type=str, default='10,2')\nparser.add_argument('--dec_hidden_dim', type=str, default='10')\nparser.add_argument('--est_hidden_dim', type=str, default='4, 10, 2')\nparser.add_argument('--dropout', action='store_true', default=0.5)\nparser.add_argument('--learning_rate', type=float, default=0.001)\nparser.add_argument('--num_epoch', type=int, default=10)\n\nif 'ipykernel_launcher' in sys.argv[0]:\n    sys.argv = [sys.argv[0]]  \n\nargs = parser.parse_args()\n\nenc_hidden_dim = args.enc_hidden_dim.split(',')\ndec_hidden_dim = args.dec_hidden_dim.split(',')\nest_hidden_dim = args.est_hidden_dim.split(',')\n\nargs.enc_hidden_dim_list = []\nargs.dec_hidden_dim_list = []\nargs.est_hidden_dim_list = []\n\nargs.enc_hidden_dim_list.append(args.input_dim)\n\nfor i in enc_hidden_dim:\n    args.enc_hidden_dim_list.append(int(i))\n\nargs.enc_hidden_dim_list\n\nargs.dec_hidden_dim_list.append(args.enc_hidden_dim_list[-1])\n\nfor i in dec_hidden_dim:\n    args.dec_hidden_dim_list.append(int(i))\n\nargs.dec_hidden_dim_list.append(args.input_dim)\n\nargs.dec_hidden_dim_list\n\nfor i in est_hidden_dim:\n    args.est_hidden_dim_list.append(int(i))\n\nargs.est_hidden_dim_list\n\nargs\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nNamespace(input_dim=280, enc_hidden_dim='10,2', dec_hidden_dim='10', est_hidden_dim='4, 10, 2', dropout=0.5, learning_rate=0.001, num_epoch=10, enc_hidden_dim_list=[280, 10, 2], dec_hidden_dim_list=[2, 10, 280], est_hidden_dim_list=[4, 10, 2])\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n### compresssion network\nclass midlayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(midlayer, self).__init__()\n        self.fc_layer   = nn.Linear(input_dim, hidden_dim)\n        self.activation = nn.Tanh()\n    \n    def forward(self, input):\n        out = self.fc_layer(input)        \n        out = self.activation(out)\n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Encoder, self).__init__()\n        \n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Linear(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n\n    def forward(self, input):\n        out = self.layer(input)\n        return out\n    \nclass Decoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Decoder, self).__init__()\n\n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(midlayer(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n    \n    def forward(self, input):\n        out = self.layer(input)\n        return out\n\nclass CompressionNet(nn.Module):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list):\n        super().__init__()\n        self.encoder = Encoder(enc_hidden_dim_list)\n        self.decoder = Decoder(dec_hidden_dim_list)\n\n        self._reconstruction_loss = nn.MSELoss()\n\n    def forward(self, input):\n        out = self.encoder(input)\n        out = self.decoder(out)\n        return out\n\n    def encode(self, input):\n        return self.encoder(input)\n\n    def decode(self, input):\n        return self.decoder(input)\n\n    def reconstuction_loss(self, input, input_target):\n        target_hat = self(input)\n        return self._reconstruction_loss(target_hat, input_target)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n### reconstructed error\neps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n\ndef relative_euclidean_distance(x1, x2, eps=eps):\n    num = torch.norm(x1 - x2, p=2, dim=1)\n    denom = torch.norm(x1, p=2, dim=1)\n    return num / torch.max(denom, eps)\n\ndef cosine_similarity(x1, x2, eps=eps):\n    dot_prod = torch.sum(x1 * x2, dim=1)\n    dist_x1 = torch.norm(x1, p=2, dim=1)\n    dist_x2 = torch.norm(x2, p=2, dim=1)\n    return dot_prod / torch.max(dist_x1*dist_x2, eps)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n### estimation network\nclass Estimation(nn.Module):\n    def __init__(self, est_hidden_dim_list):\n        super().__init__()\n        \n        layer_list = []\n        for i in range(len(est_hidden_dim_list)-2):\n            layer_list.append(midlayer(est_hidden_dim_list[i], est_hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Dropout(p=0.5))\n        layer_list.append(nn.Linear(est_hidden_dim_list[-2], est_hidden_dim_list[-1]))\n        layer_list.append(nn.Softmax())\n        self.net = nn.Sequential(*layer_list)\n        \n    def forward(self, input):\n        out = self.net(input)\n        return out\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n### Mixture\nclass Mixture(nn.Module):\n    def __init__(self, latent_dimension):\n        super().__init__()\n        self.latent_dimension = latent_dimension\n\n        self.Phi    = np.random.random([1])\n        self.Phi    = torch.from_numpy(self.Phi).float()\n        self.Phi    = nn.Parameter(self.Phi, requires_grad = False)\n\n        self.mu     = 2.*np.random.random([latent_dimension]) - 0.5\n        self.mu     = torch.from_numpy(self.mu).float()\n        self.mu     = nn.Parameter(self.mu, requires_grad = False)\n\n        self.Sigma  = np.eye(latent_dimension, latent_dimension)\n        self.Sigma  = torch.from_numpy(self.Sigma).float()\n        self.Sigma  = nn.Parameter(self.Sigma, requires_grad = False)\n        \n        self.eps_Sigma  = torch.FloatTensor(np.diag([1.e-8 for _ in range(latent_dimension)]))\n\n    def forward(self, est_inputs, with_log = True):\n        batch_size, _   = est_inputs.shape\n        out_values  = []\n        inv_sigma   = torch.inverse(self.Sigma)\n        det_sigma   = np.linalg.det(self.Sigma.data.cpu().numpy())\n        det_sigma   = torch.from_numpy(det_sigma.reshape([1])).float()\n        det_sigma   = torch.autograd.Variable(det_sigma)\n        for est_input in est_inputs:\n            diff    = (est_input - self.mu).view(-1,1)\n            out     = -0.5 * torch.mm(torch.mm(diff.view(1,-1), inv_sigma), diff)\n            out     = (self.Phi * torch.exp(out)) / torch.sqrt(2. * np.pi * det_sigma)\n            if with_log:\n                out = -torch.log(out)\n            out_values.append(float(out.data.cpu().numpy()))\n\n        out = torch.autograd.Variable(torch.FloatTensor(out_values))\n        return out\n    \n    def _update_parameters(self, samples, affiliations):\n        if not self.training:\n            return\n\n        batch_size, _ = samples.shape\n\n        # Updating phi.\n        phi = torch.mean(affiliations)\n        self.Phi.data = phi.data\n\n        # Updating mu.\n        num = 0.\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            num     += gamma_i * z_i\n        \n        denom        = torch.sum(affiliations)\n        self.mu.data = (num / denom).data\n\n        # Updating Sigma.\n        mu  = self.mu\n        num = None\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            diff    = (z_i - mu).view(-1, 1)\n            to_add  = gamma_i * torch.mm(diff, diff.view(1, -1))\n            if num is None:\n                num = to_add\n            else:\n                num += to_add\n\n        denom           = torch.sum(affiliations)\n        self.Sigma.data = (num / denom).data + self.eps_Sigma\n\n\nclass GMM(nn.Module):\n    def __init__(self, num_mixtures, latent_dimension):\n        super().__init__()\n        self.num_mixtures       = num_mixtures\n        self.latent_dimension   = latent_dimension\n\n        mixtures        = [Mixture(latent_dimension) for _ in range(num_mixtures)]\n        self.mixtures   = nn.ModuleList(mixtures)\n    \n    def forward(self, est_inputs):\n        out = None\n        for mixture in self.mixtures:\n            to_add  = mixture(est_inputs, with_log = False)\n            if out is None:\n                out = to_add\n            else:\n                out += to_add\n        return -torch.log(out)\n    \n    def _update_mixtures_parameters(self, samples, mixtures_affiliations):\n        if not self.training:\n            return\n\n        for i, mixture in enumerate(self.mixtures):\n            affiliations = mixtures_affiliations[:, i]\n            mixture._update_parameters(samples, affiliations)\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n### model\nclass DAGMM(nn.Module):\n    def __init__(self, compression_module, estimation_module, gmm_module):\n        super().__init__()\n\n        self.compressor = compression_module\n        self.estimator  = estimation_module\n        self.gmm        = gmm_module\n\n    def forward(self, input):\n        encoded = self.compressor.encode(input)\n        decoded = self.compressor.decode(encoded)\n\n        relative_ed     = relative_euclidean_distance(input, decoded)\n        cosine_sim      = cosine_similarity(input, decoded)\n\n        relative_ed     = relative_ed.view(-1, 1)\n        cosine_sim      = relative_ed.view(-1, 1)\n        latent_vectors  = torch.cat([encoded, relative_ed, cosine_sim], dim=1)\n\n        if self.training:\n            mixtures_affiliations = self.estimator(latent_vectors)\n            self.gmm._update_mixtures_parameters(latent_vectors,\n                                                 mixtures_affiliations)\n        return self.gmm(latent_vectors)\n\n\nclass DAGMMArrhythmia(DAGMM):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list, est_hidden_dim_list):\n        compressor  = CompressionNet(enc_hidden_dim_list, dec_hidden_dim_list)\n        estimator   = Estimation(est_hidden_dim_list)\n        gmm = GMM(num_mixtures=2, latent_dimension=4)\n\n        super().__init__(compression_module = compressor,\n                         estimation_module  = estimator,\n                         gmm_module         = gmm)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n### tests\ndef test_dagmm():\n    net = DAGMMArrhythmia(args.enc_hidden_dim_list, args.dec_hidden_dim_list, args.est_hidden_dim_list)\n    out = net(data_array)\n    print(out)\n\ndef convert_to_var(input):\n    out = torch.from_numpy(input).float()\n    out = torch.autograd.Variable(out)\n    return out\n\ndef test_update_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n    mix              = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    affiliations     = np.random.random([batch_size])\n    latent_vectors   = convert_to_var(latent_vectors)\n    affiliations     = convert_to_var(affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n    mix.train()\n    mix._update_parameters(latent_vectors, affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n\ndef test_forward_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n\n    mix = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    latent_vectors   = convert_to_var(latent_vectors)\n\n    mix.train()\n    out = mix(latent_vectors)\n    print(out)\n\n\ndef test_update_gmm():\n    batch_size      = int(5)\n    latent_dimension= 7\n    num_mixtures    = 2\n\n    gmm = GMM(num_mixtures, latent_dimension)\n\n    latent_vectors  = np.random.random([batch_size, latent_dimension])\n    latent_vectors  = convert_to_var(latent_vectors)\n\n    affiliations    = np.random.random([batch_size, num_mixtures])\n    affiliations    = convert_to_var(affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n    gmm.train()\n    gmm._update_mixtures_parameters(latent_vectors, affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nif __name__ == '__main__':\n    test_update_mixture()\n    test_forward_mixture()\n    test_update_gmm()\n    test_dagmm()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([0.6708])\nParameter containing:\ntensor([ 0.0744, -0.3463,  0.1928, -0.0517,  0.2910,  0.0668,  1.1879])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.5546)\nParameter containing:\ntensor([0.1851, 0.2101, 0.4965, 0.5223, 0.4495, 0.7594, 0.3953])\nParameter containing:\ntensor([[ 0.0318,  0.0050,  0.0162,  0.0379,  0.0040,  0.0040,  0.0056],\n        [ 0.0050,  0.0393, -0.0210,  0.0103, -0.0568,  0.0220,  0.0469],\n        [ 0.0162, -0.0210,  0.1608,  0.0731,  0.0986, -0.0601, -0.0862],\n        [ 0.0379,  0.0103,  0.0731,  0.0693,  0.0219, -0.0098, -0.0135],\n        [ 0.0040, -0.0568,  0.0986,  0.0219,  0.1173, -0.0607, -0.0939],\n        [ 0.0040,  0.0220, -0.0601, -0.0098, -0.0607,  0.0441,  0.0458],\n        [ 0.0056,  0.0469, -0.0862, -0.0135, -0.0939,  0.0458,  0.0832]])\ntensor([1.8491, 2.2013, 2.1552, 1.5288, 1.8039])\nParameter containing:\ntensor([0.2684])\nParameter containing:\ntensor([ 0.8169, -0.3349,  0.6617,  0.7210,  1.1626,  0.4094,  0.2853])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor([0.9417])\nParameter containing:\ntensor([ 0.1645,  0.1663, -0.3662,  1.1244, -0.4419, -0.3331,  0.1235])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.6033)\nParameter containing:\ntensor([0.4998, 0.3665, 0.3353, 0.4332, 0.5558, 0.7133, 0.5177])\nParameter containing:\ntensor([[ 0.0387, -0.0469, -0.0093,  0.0224,  0.0045,  0.0055,  0.0054],\n        [-0.0469,  0.0889,  0.0099, -0.0315, -0.0301, -0.0062, -0.0251],\n        [-0.0093,  0.0099,  0.0118,  0.0068,  0.0017,  0.0017, -0.0159],\n        [ 0.0224, -0.0315,  0.0068,  0.0309,  0.0081,  0.0077, -0.0151],\n        [ 0.0045, -0.0301,  0.0017,  0.0081,  0.0198,  0.0008,  0.0121],\n        [ 0.0055, -0.0062,  0.0017,  0.0077,  0.0008,  0.0020, -0.0049],\n        [ 0.0054, -0.0251, -0.0159, -0.0151,  0.0121, -0.0049,  0.0373]])\nParameter containing:\ntensor(0.5910)\nParameter containing:\ntensor([0.4718, 0.5168, 0.3861, 0.3428, 0.4769, 0.6866, 0.4324])\nParameter containing:\ntensor([[ 0.0259, -0.0333, -0.0069,  0.0161,  0.0043,  0.0040,  0.0049],\n        [-0.0333,  0.0850,  0.0164, -0.0423, -0.0352, -0.0108, -0.0309],\n        [-0.0069,  0.0164,  0.0149, -0.0047, -0.0039, -0.0021, -0.0186],\n        [ 0.0161, -0.0423, -0.0047,  0.0356,  0.0181,  0.0098,  0.0045],\n        [ 0.0043, -0.0352, -0.0039,  0.0181,  0.0221,  0.0044,  0.0155],\n        [ 0.0040, -0.0108, -0.0021,  0.0098,  0.0044,  0.0029,  0.0016],\n        [ 0.0049, -0.0309, -0.0186,  0.0045,  0.0155,  0.0016,  0.0334]])\ntensor([-20.2235, -19.5796, -18.2748, -20.0366, -20.2120,  -3.3468, -20.2282,\n        -19.9478, -20.1908, -20.2319, -20.1559, -20.0203, -19.9910, -20.0385,\n        -19.3059, -20.0160, -19.0049, -15.0871, -20.1826, -20.1166, -17.8514,\n        -19.2502, -20.0698, -20.2161, -20.1287, -20.0696, -18.3994, -20.1507,\n        -15.5779, -12.5161, -19.8662, -19.4357, -19.5880, -20.1350, -20.0799,\n        -20.0312, -20.1304, -20.2003, -20.0427, -18.5765, -19.6710, -19.9432,\n        -19.9821, -19.2762, -20.0808, -17.4431, -20.2103, -19.9602, -20.1087,\n        -19.9409, -20.0862, -19.5724, -20.2076, -17.5777, -20.0907, -19.2899,\n        -19.6751, -19.9591, -20.2265, -19.8545, -19.7321, -20.0437, -19.8994,\n        -20.1703, -18.1992, -20.1769, -19.8955, -19.7147, -19.9189, -19.6573,\n        -19.6324, -19.9687, -19.9866, -20.2188, -20.1788, -19.6756, -15.5464,\n        -20.0430, -18.4116, -19.9975, -19.6549, -19.7680, -19.9588, -20.0924,\n        -20.0798, -17.8145, -20.2242, -19.5241, -18.2970, -20.1807, -17.3323,\n        -19.6756, -19.9130, -19.7627, -19.0984, -16.9743, -18.3699, -19.6322,\n        -19.8802, -13.0792, -19.8571, -17.5672, -11.8144, -19.0541, -20.1147,\n        -19.9259, -10.1497, -20.1374, -13.6075, -19.9318, -17.3490, -20.1207,\n        -20.2279, -18.4644, -20.2026, -20.2200, -18.1918, -17.3313, -20.1266,\n        -20.1501, -19.5323, -18.5575, -19.6269, -19.7861, -19.8430, -19.2241,\n        -20.1233, -20.2272, -19.5726, -20.0968, -17.0089, -20.1908, -20.2254,\n        -15.1696, -20.1749, -20.0739, -19.8494, -19.4763, -20.1868, -20.0521,\n        -20.1850, -20.0310, -19.9391, -19.6083, -19.8503, -20.1680, -18.7109,\n        -20.0696, -19.8996, -20.2076, -20.1285, -20.1943, -19.5300, -19.8305,\n        -19.9620, -19.5923, -20.2098, -20.2126, -19.1490, -19.7566, -20.0215,\n        -20.2253, -20.2282, -20.2123, -20.0022, -20.2195, -19.7390, -20.0973,\n        -20.2036, -20.2148, -17.6385, -19.1051, -20.0537, -19.6717, -18.1206,\n        -13.3478, -20.2282, -20.0395, -19.0924, -16.0135, -19.9751, -20.1905,\n        -18.7753, -18.6293, -18.7644, -20.0694, -20.2277, -16.1424, -19.3795,\n        -12.5818, -18.5242, -20.0463, -20.1280, -19.6429, -19.7690, -19.9775,\n        -20.2092, -19.8706, -11.7017, -19.5606, -20.2114, -19.5944,  -4.7972,\n        -20.2233, -18.2674, -20.1300, -19.7335, -20.0928, -19.6151, -20.1276,\n        -19.2420, -19.9885, -20.2277, -17.3811, -20.2235, -20.0476, -18.0080,\n        -18.6992, -19.0452, -18.5626, -18.9380, -20.2170, -19.9416, -19.5714,\n        -20.2123, -15.2158, -20.0069, -19.6128, -20.2222, -20.2066, -19.9658,\n        -19.9399, -20.2157, -19.9143, -20.1029, -12.2647, -20.0939, -19.8378,\n        -19.9069, -18.3312, -20.1544, -12.2160, -19.5462, -17.9280, -20.0103,\n        -12.3935, -20.0941, -19.3605, -17.0206, -19.7714, -20.1615, -17.3467,\n        -12.4248, -12.6264, -16.0423, -20.2009, -18.8140, -12.9780, -20.1700,\n        -20.1144, -18.0521, -19.5850, -20.1609, -20.2211, -19.2474, -19.4934,\n        -20.2263, -20.2281, -20.2141, -19.8640, -20.2276, -18.4989, -20.1412,\n        -20.2073, -20.2119, -20.0940, -19.2913, -19.7819, -20.0905, -19.4009,\n        -20.1439, -20.2230, -20.1651, -20.2034,  -3.1711, -14.8115, -20.1249,\n        -20.2278, -19.2863, -19.2409, -20.1818, -18.9364, -19.7598, -19.9828,\n        -19.8661, -19.4412, -14.2111, -19.1426, -18.7352, -20.1078, -10.2869,\n        -19.4248, -19.9825, -14.9699, -19.1416, -19.9673, -20.1040, -18.8180,\n        -17.7334, -20.2227, -18.6138, -19.6634, -19.5755, -20.2135, -20.1384,\n        -19.1177, -17.2645, -19.8158, -20.1971, -19.5122, -16.0656, -19.9423,\n        -17.3143, -14.3509, -19.8850, -19.9810, -19.0178, -11.9086, -18.4911,\n        -17.3414, -19.4352, -19.6163, -19.8149, -19.8486, -20.0638, -20.1256,\n        -18.8305, -20.0627, -20.1914, -20.1836, -19.9506, -20.1327, -20.1412,\n        -20.1764, -17.9564, -20.2157, -19.9270, -19.8813, -14.8900, -18.4771,\n        -15.0317, -16.0351, -16.3937, -19.8136, -17.3297, -20.1020, -15.2023,\n        -19.8852, -20.1410, -15.1101, -20.0787, -15.4456, -18.9437, -20.2281,\n        -20.1290, -19.0569, -11.3157, -19.6499, -17.0891, -11.9564, -20.1864,\n        -18.3846, -20.2272, -20.0108, -19.9568, -20.1103, -12.0232, -18.8302,\n        -20.1996, -19.6827, -17.4583, -13.0812, -20.1706, -19.1465, -20.2258,\n        -19.6770, -11.9548, -19.2042, -12.0349, -19.1939, -19.4332, -18.3247,\n        -20.2003, -18.4369, -20.0828, -14.0724, -20.2177, -20.1335, -20.2074,\n        -20.0335, -19.9797, -12.7783, -19.4577, -14.7200, -19.6989, -20.0652,\n        -19.8943, -19.4418, -19.9583, -18.4946, -19.9660, -20.1008, -19.9310,\n        -20.1120, -20.0183, -19.1694, -19.2284, -10.0634, -20.1168, -20.0106,\n         -4.8029, -20.0988, -20.2199, -19.4736, -12.5663, -20.2186, -19.4957,\n        -20.1714, -17.5186, -20.1014, -11.8549, -20.2172, -19.9964, -12.3047,\n        -20.1148, -20.0857, -20.2281, -19.9667, -20.0292, -20.1981, -20.0960,\n        -19.1063, -20.0809, -19.7993, -20.0791, -19.0565, -20.1956, -17.5403,\n        -20.1692, -17.5986, -20.1645, -19.9429])\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\UOS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning:\n\nImplicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n\n```\n:::\n:::\n\n\n### Ref\n- https://openreview.net/forum?id=BJJLHbb0-\n\n",
    "supporting": [
      "2023-10-15-arrhythmia_files"
    ],
    "filters": [],
    "includes": {}
  }
}