{
  "hash": "831247fe92f57bc7b70a5046f4292127",
  "result": {
    "markdown": "---\ntitle: \"**[DAGMM]** DAGMM: for arrhythmia data set\"\nauthor: \"kione kim\"\ndate: \"10/19/2023\"\nbibliography: dagmm.bib\n---\n\n## Deep Autoencoding Gaussian Mixture Model for Arrhythmia dataset\n\n::: {#6c22e6aa .cell execution_count=1}\n``` {.python .cell-code}\n### imports\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport sys\n```\n:::\n\n\n::: {#ed59372b .cell execution_count=2}\n``` {.python .cell-code}\n### data 파일\nfile_path = 'C:\\\\Users\\\\UOS\\\\Desktop\\\\연구\\\\5. 데이터\\\\data\\\\arrhythmia\\\\arrhythmia.data'\n\ndf = pd.read_csv(file_path, header=None)\ndf = df.replace('?', 0)\ndf = df.astype('float64')\n\ndata_array = df.values\ndata_array = torch.autograd.Variable(torch.from_numpy(data_array).float())\ndata_array.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntorch.Size([452, 280])\n```\n:::\n:::\n\n\n::: {#fe64decf .cell execution_count=3}\n``` {.python .cell-code}\nparser = argparse.ArgumentParser(description='parser for argparse test')\n\nparser.add_argument('--input_dim', type=int, default=data_array.shape[-1])\nparser.add_argument('--enc_hidden_dim', type=str, default='10,2')\nparser.add_argument('--dec_hidden_dim', type=str, default='10')\nparser.add_argument('--est_hidden_dim', type=str, default='4, 10, 2')\nparser.add_argument('--dropout', action='store_true', default=0.5)\nparser.add_argument('--learning_rate', type=float, default=0.001)\nparser.add_argument('--num_epoch', type=int, default=10)\n\nif 'ipykernel_launcher' in sys.argv[0]:\n    sys.argv = [sys.argv[0]]  \n\nargs = parser.parse_args()\n\nenc_hidden_dim = args.enc_hidden_dim.split(',')\ndec_hidden_dim = args.dec_hidden_dim.split(',')\nest_hidden_dim = args.est_hidden_dim.split(',')\n\nargs.enc_hidden_dim_list = []\nargs.dec_hidden_dim_list = []\nargs.est_hidden_dim_list = []\n\nargs.enc_hidden_dim_list.append(args.input_dim)\n\nfor i in enc_hidden_dim:\n    args.enc_hidden_dim_list.append(int(i))\n\nargs.enc_hidden_dim_list\n\nargs.dec_hidden_dim_list.append(args.enc_hidden_dim_list[-1])\n\nfor i in dec_hidden_dim:\n    args.dec_hidden_dim_list.append(int(i))\n\nargs.dec_hidden_dim_list.append(args.input_dim)\n\nargs.dec_hidden_dim_list\n\nfor i in est_hidden_dim:\n    args.est_hidden_dim_list.append(int(i))\n\nargs.est_hidden_dim_list\n\nargs\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nNamespace(input_dim=280, enc_hidden_dim='10,2', dec_hidden_dim='10', est_hidden_dim='4, 10, 2', dropout=0.5, learning_rate=0.001, num_epoch=10, enc_hidden_dim_list=[280, 10, 2], dec_hidden_dim_list=[2, 10, 280], est_hidden_dim_list=[4, 10, 2])\n```\n:::\n:::\n\n\n::: {#3920814b .cell execution_count=4}\n``` {.python .cell-code}\n### compresssion network\nclass midlayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(midlayer, self).__init__()\n        self.fc_layer   = nn.Linear(input_dim, hidden_dim)\n        self.activation = nn.Tanh()\n    \n    def forward(self, input):\n        out = self.fc_layer(input)        \n        out = self.activation(out)\n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Encoder, self).__init__()\n        \n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Linear(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n\n    def forward(self, input):\n        out = self.layer(input)\n        return out\n    \nclass Decoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Decoder, self).__init__()\n\n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(midlayer(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n    \n    def forward(self, input):\n        out = self.layer(input)\n        return out\n\nclass CompressionNet(nn.Module):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list):\n        super().__init__()\n        self.encoder = Encoder(enc_hidden_dim_list)\n        self.decoder = Decoder(dec_hidden_dim_list)\n\n        self._reconstruction_loss = nn.MSELoss()\n\n    def forward(self, input):\n        out = self.encoder(input)\n        out = self.decoder(out)\n        return out\n\n    def encode(self, input):\n        return self.encoder(input)\n\n    def decode(self, input):\n        return self.decoder(input)\n\n    def reconstuction_loss(self, input, input_target):\n        target_hat = self(input)\n        return self._reconstruction_loss(target_hat, input_target)\n```\n:::\n\n\n::: {#12196a2a .cell execution_count=5}\n``` {.python .cell-code}\n### reconstructed error\neps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n\ndef relative_euclidean_distance(x1, x2, eps=eps):\n    num = torch.norm(x1 - x2, p=2, dim=1)\n    denom = torch.norm(x1, p=2, dim=1)\n    return num / torch.max(denom, eps)\n\ndef cosine_similarity(x1, x2, eps=eps):\n    dot_prod = torch.sum(x1 * x2, dim=1)\n    dist_x1 = torch.norm(x1, p=2, dim=1)\n    dist_x2 = torch.norm(x2, p=2, dim=1)\n    return dot_prod / torch.max(dist_x1*dist_x2, eps)\n```\n:::\n\n\n::: {#b6a04e85 .cell execution_count=6}\n``` {.python .cell-code}\n### estimation network\nclass Estimation(nn.Module):\n    def __init__(self, est_hidden_dim_list):\n        super().__init__()\n        \n        layer_list = []\n        for i in range(len(est_hidden_dim_list)-2):\n            layer_list.append(midlayer(est_hidden_dim_list[i], est_hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Dropout(p=0.5))\n        layer_list.append(nn.Linear(est_hidden_dim_list[-2], est_hidden_dim_list[-1]))\n        layer_list.append(nn.Softmax())\n        self.net = nn.Sequential(*layer_list)\n        \n    def forward(self, input):\n        out = self.net(input)\n        return out\n```\n:::\n\n\n::: {#e60ab917 .cell execution_count=7}\n``` {.python .cell-code}\n### Mixture\nclass Mixture(nn.Module):\n    def __init__(self, latent_dimension):\n        super().__init__()\n        self.latent_dimension = latent_dimension\n\n        self.Phi    = np.random.random([1])\n        self.Phi    = torch.from_numpy(self.Phi).float()\n        self.Phi    = nn.Parameter(self.Phi, requires_grad = False)\n\n        self.mu     = 2.*np.random.random([latent_dimension]) - 0.5\n        self.mu     = torch.from_numpy(self.mu).float()\n        self.mu     = nn.Parameter(self.mu, requires_grad = False)\n\n        self.Sigma  = np.eye(latent_dimension, latent_dimension)\n        self.Sigma  = torch.from_numpy(self.Sigma).float()\n        self.Sigma  = nn.Parameter(self.Sigma, requires_grad = False)\n        \n        self.eps_Sigma  = torch.FloatTensor(np.diag([1.e-8 for _ in range(latent_dimension)]))\n\n    def forward(self, est_inputs, with_log = True):\n        batch_size, _   = est_inputs.shape\n        out_values  = []\n        inv_sigma   = torch.inverse(self.Sigma)\n        det_sigma   = np.linalg.det(self.Sigma.data.cpu().numpy())\n        det_sigma   = torch.from_numpy(det_sigma.reshape([1])).float()\n        det_sigma   = torch.autograd.Variable(det_sigma)\n        for est_input in est_inputs:\n            diff    = (est_input - self.mu).view(-1,1)\n            out     = -0.5 * torch.mm(torch.mm(diff.view(1,-1), inv_sigma), diff)\n            out     = (self.Phi * torch.exp(out)) / torch.sqrt(2. * np.pi * det_sigma)\n            if with_log:\n                out = -torch.log(out)\n            out_values.append(float(out.data.cpu().numpy()))\n\n        out = torch.autograd.Variable(torch.FloatTensor(out_values))\n        return out\n    \n    def _update_parameters(self, samples, affiliations):\n        if not self.training:\n            return\n\n        batch_size, _ = samples.shape\n\n        # Updating phi.\n        phi = torch.mean(affiliations)\n        self.Phi.data = phi.data\n\n        # Updating mu.\n        num = 0.\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            num     += gamma_i * z_i\n        \n        denom        = torch.sum(affiliations)\n        self.mu.data = (num / denom).data\n\n        # Updating Sigma.\n        mu  = self.mu\n        num = None\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            diff    = (z_i - mu).view(-1, 1)\n            to_add  = gamma_i * torch.mm(diff, diff.view(1, -1))\n            if num is None:\n                num = to_add\n            else:\n                num += to_add\n\n        denom           = torch.sum(affiliations)\n        self.Sigma.data = (num / denom).data + self.eps_Sigma\n\n\nclass GMM(nn.Module):\n    def __init__(self, num_mixtures, latent_dimension):\n        super().__init__()\n        self.num_mixtures       = num_mixtures\n        self.latent_dimension   = latent_dimension\n\n        mixtures        = [Mixture(latent_dimension) for _ in range(num_mixtures)]\n        self.mixtures   = nn.ModuleList(mixtures)\n    \n    def forward(self, est_inputs):\n        out = None\n        for mixture in self.mixtures:\n            to_add  = mixture(est_inputs, with_log = False)\n            if out is None:\n                out = to_add\n            else:\n                out += to_add\n        return -torch.log(out)\n    \n    def _update_mixtures_parameters(self, samples, mixtures_affiliations):\n        if not self.training:\n            return\n\n        for i, mixture in enumerate(self.mixtures):\n            affiliations = mixtures_affiliations[:, i]\n            mixture._update_parameters(samples, affiliations)\n```\n:::\n\n\n::: {#1b4dc148 .cell execution_count=8}\n``` {.python .cell-code}\n### model\nclass DAGMM(nn.Module):\n    def __init__(self, compression_module, estimation_module, gmm_module):\n        super().__init__()\n\n        self.compressor = compression_module\n        self.estimator  = estimation_module\n        self.gmm        = gmm_module\n\n    def forward(self, input):\n        encoded = self.compressor.encode(input)\n        decoded = self.compressor.decode(encoded)\n\n        relative_ed     = relative_euclidean_distance(input, decoded)\n        cosine_sim      = cosine_similarity(input, decoded)\n\n        relative_ed     = relative_ed.view(-1, 1)\n        cosine_sim      = relative_ed.view(-1, 1)\n        latent_vectors  = torch.cat([encoded, relative_ed, cosine_sim], dim=1)\n\n        if self.training:\n            mixtures_affiliations = self.estimator(latent_vectors)\n            self.gmm._update_mixtures_parameters(latent_vectors,\n                                                 mixtures_affiliations)\n        return self.gmm(latent_vectors)\n\n\nclass DAGMMArrhythmia(DAGMM):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list, est_hidden_dim_list):\n        compressor  = CompressionNet(enc_hidden_dim_list, dec_hidden_dim_list)\n        estimator   = Estimation(est_hidden_dim_list)\n        gmm = GMM(num_mixtures=2, latent_dimension=4)\n\n        super().__init__(compression_module = compressor,\n                         estimation_module  = estimator,\n                         gmm_module         = gmm)\n```\n:::\n\n\n::: {#95e720b2 .cell execution_count=9}\n``` {.python .cell-code}\n### tests\ndef test_dagmm():\n    net = DAGMMArrhythmia(args.enc_hidden_dim_list, args.dec_hidden_dim_list, args.est_hidden_dim_list)\n    out = net(data_array)\n    print(out)\n\ndef convert_to_var(input):\n    out = torch.from_numpy(input).float()\n    out = torch.autograd.Variable(out)\n    return out\n\ndef test_update_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n    mix              = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    affiliations     = np.random.random([batch_size])\n    latent_vectors   = convert_to_var(latent_vectors)\n    affiliations     = convert_to_var(affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n    mix.train()\n    mix._update_parameters(latent_vectors, affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n\ndef test_forward_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n\n    mix = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    latent_vectors   = convert_to_var(latent_vectors)\n\n    mix.train()\n    out = mix(latent_vectors)\n    print(out)\n\n\ndef test_update_gmm():\n    batch_size      = int(5)\n    latent_dimension= 7\n    num_mixtures    = 2\n\n    gmm = GMM(num_mixtures, latent_dimension)\n\n    latent_vectors  = np.random.random([batch_size, latent_dimension])\n    latent_vectors  = convert_to_var(latent_vectors)\n\n    affiliations    = np.random.random([batch_size, num_mixtures])\n    affiliations    = convert_to_var(affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n    gmm.train()\n    gmm._update_mixtures_parameters(latent_vectors, affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n```\n:::\n\n\n::: {#60140ca1 .cell execution_count=10}\n``` {.python .cell-code}\nif __name__ == '__main__':\n    test_update_mixture()\n    test_forward_mixture()\n    test_update_gmm()\n    test_dagmm()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor([0.7155])\nParameter containing:\ntensor([ 0.3481,  1.3280, -0.2945,  0.7378,  0.9525,  1.0533,  0.3526])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.6394)\nParameter containing:\ntensor([0.4437, 0.4980, 0.5469, 0.2490, 0.6153, 0.5041, 0.6236])\nParameter containing:\ntensor([[ 0.1176,  0.0470, -0.0460, -0.0388,  0.0193, -0.0823,  0.0473],\n        [ 0.0470,  0.1421, -0.0949, -0.0216,  0.0253, -0.0732,  0.0108],\n        [-0.0460, -0.0949,  0.0658,  0.0214, -0.0236,  0.0539, -0.0142],\n        [-0.0388, -0.0216,  0.0214,  0.0536, -0.0543,  0.0057, -0.0472],\n        [ 0.0193,  0.0253, -0.0236, -0.0543,  0.0808,  0.0244,  0.0325],\n        [-0.0823, -0.0732,  0.0539,  0.0057,  0.0244,  0.0975, -0.0210],\n        [ 0.0473,  0.0108, -0.0142, -0.0472,  0.0325, -0.0210,  0.0510]])\ntensor([1.6205, 1.5219, 2.1690, 2.0467, 2.0965])\nParameter containing:\ntensor([0.0769])\nParameter containing:\ntensor([ 1.2234,  0.9047,  1.0385, -0.3509,  0.9589,  0.1277,  1.1293])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor([0.6805])\nParameter containing:\ntensor([ 0.6439,  1.2563,  0.6759,  1.3079,  0.9966, -0.4996,  0.4709])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.4160)\nParameter containing:\ntensor([0.3649, 0.4907, 0.1545, 0.6987, 0.6694, 0.6663, 0.7361])\nParameter containing:\ntensor([[ 0.0573, -0.0107,  0.0043, -0.0163, -0.0796, -0.0312,  0.0385],\n        [-0.0107,  0.0176,  0.0093, -0.0143,  0.0182,  0.0053, -0.0376],\n        [ 0.0043,  0.0093,  0.0094, -0.0130,  0.0004,  0.0046, -0.0112],\n        [-0.0163, -0.0143, -0.0130,  0.0243,  0.0180,  0.0084,  0.0222],\n        [-0.0796,  0.0182,  0.0004,  0.0180,  0.1181,  0.0554, -0.0506],\n        [-0.0312,  0.0053,  0.0046,  0.0084,  0.0554,  0.0390, -0.0030],\n        [ 0.0385, -0.0376, -0.0112,  0.0222, -0.0506, -0.0030,  0.0983]])\nParameter containing:\ntensor(0.4811)\nParameter containing:\ntensor([0.4721, 0.5716, 0.2574, 0.5500, 0.5914, 0.6908, 0.6772])\nParameter containing:\ntensor([[ 0.0468, -0.0195, -0.0092, -0.0013, -0.0769, -0.0442,  0.0378],\n        [-0.0195,  0.0397,  0.0291, -0.0358,  0.0469,  0.0312, -0.0668],\n        [-0.0092,  0.0291,  0.0236, -0.0301,  0.0296,  0.0215, -0.0463],\n        [-0.0013, -0.0358, -0.0301,  0.0433, -0.0170, -0.0149,  0.0566],\n        [-0.0769,  0.0469,  0.0296, -0.0170,  0.1375,  0.0829, -0.0827],\n        [-0.0442,  0.0312,  0.0215, -0.0149,  0.0829,  0.0526, -0.0519],\n        [ 0.0378, -0.0668, -0.0463,  0.0566, -0.0827, -0.0519,  0.1165]])\ntensor([-18.6494, -18.7065, -18.5218, -18.2760, -16.5461, -18.4331, -18.5868,\n        -15.8156, -18.7551, -18.4427, -17.2782, -18.6807, -17.3284, -18.6346,\n        -18.5811, -18.2465, -17.5672, -18.6877, -15.6259, -16.6864, -18.7658,\n        -18.7989, -18.7980, -18.7470, -18.3945, -17.1276, -18.6308, -17.6492,\n        -14.2039,  -9.8806, -17.3291, -18.1739, -15.3988, -18.7924, -18.4635,\n        -18.7003, -18.4222, -18.7047, -18.6428, -15.1316, -17.7366, -15.7518,\n        -18.7173, -18.7942, -17.1102, -17.2144, -18.4361, -18.4531, -18.7714,\n        -18.2113, -18.4661, -18.5559, -18.3666, -17.9165, -17.6876, -18.6707,\n        -15.5488, -18.6766, -16.4268, -18.7577, -16.8919, -18.5343, -18.4669,\n        -18.2454, -16.6817, -17.5418, -18.8523, -18.7414, -18.7990, -18.0853,\n        -18.6867, -18.2713, -18.4839, -18.4888, -17.8579, -18.7228, -16.6142,\n        -17.3071, -18.5489, -18.7619, -16.4048, -18.2643, -17.2369, -17.9029,\n        -18.7981, -16.4739, -17.4128, -18.6774, -14.7842, -16.2378, -16.6147,\n        -18.0598, -17.3437, -18.7497,  -4.7673, -18.7809, -18.7396, -18.7162,\n        -17.0204, -13.4689, -18.2217, -18.7629, -17.1104, -18.5303, -18.4064,\n        -16.3850, -18.2494, -18.6735, -11.2840, -18.4177, -18.8369, -18.5555,\n        -18.4607, -18.5695, -18.7671, -18.7722, -18.3865, -16.6423, -18.2743,\n        -18.6226, -18.7906, -18.7842, -17.9270, -18.7985, -17.7386, -18.6350,\n        -18.7996, -18.7358, -18.6483, -17.1500, -18.3918, -18.7886, -16.2725,\n        -18.5582, -18.6488, -18.2305, -18.7826, -17.9677, -16.9637, -18.7198,\n        -18.7641,  -6.8419, -17.1598, -18.6097, -18.7830, -18.6599, -18.7381,\n        -18.1567, -18.8018, -18.1897, -18.3860, -18.1739, -18.6214, -18.2990,\n        -18.6964, -18.2776, -17.9713, -18.2089, -18.6853, -14.2934, -18.7617,\n        -17.1575, -18.7952, -18.1571, -18.7262, -18.5381, -18.1508, -18.4631,\n        -18.6110, -18.4009, -18.7494, -17.3691, -18.7625, -18.7864, -18.0790,\n        -18.7438, -18.4664, -18.7994, -18.7764, -15.7200, -18.7910, -18.5761,\n        -18.0935, -15.9461, -17.3284, -17.5282, -15.6093, -14.8416, -17.2021,\n        -15.2534, -17.3038, -18.7790, -17.8985, -17.5331, -18.7564, -17.8515,\n        -18.7664, -18.5696, -14.1787, -18.7868, -18.6446, -18.7162, -17.8207,\n        -18.2635, -15.1001, -18.4482, -18.5630, -17.8589, -17.6038, -18.7115,\n        -16.8247, -18.7974, -18.2153, -18.5361, -17.8881, -18.7765, -16.9380,\n        -16.6483, -17.1497, -16.4136, -18.3925, -16.7048, -18.7806, -18.5835,\n        -18.7635, -16.9929, -18.7976, -18.2312, -18.8004, -18.6971, -18.7720,\n        -17.7849, -18.5763, -18.3864, -18.7955,  -9.4280, -18.7432, -16.9201,\n        -18.6868, -18.5883, -18.5991, -13.6892, -17.9104, -16.4287, -18.6977,\n        -17.6347, -18.7168, -18.7444, -13.4912, -18.4278, -16.8399, -18.7587,\n        -15.7385, -15.6600, -13.0264, -18.7992, -14.9001, -16.9644, -18.7453,\n        -18.7270, -18.7952, -18.7877, -18.0236, -18.7986, -18.1004, -18.7726,\n        -18.5079, -17.6824, -17.2052, -18.5166, -18.6281, -18.7308, -17.6857,\n        -18.4363, -18.5452, -18.8410, -18.7989, -18.7542, -18.4494, -18.6632,\n        -18.8815, -18.7911, -18.7322, -17.2085, -16.5029, -17.3144, -18.6507,\n        -18.7592, -18.6450, -18.6312, -16.0794, -18.7870, -16.4452, -14.1248,\n        -16.0788, -18.7696, -16.2945, -16.6598, -13.8127, -16.9027, -14.8126,\n        -17.4361, -17.2378, -16.0892, -16.4074, -18.6142, -16.5308, -18.6834,\n        -15.2512, -18.7322, -13.7198, -18.8765, -14.4151, -13.9006, -18.7959,\n        -18.7240,  -6.2811, -18.6030, -18.4937, -14.1646, -17.2608, -18.7974,\n        -17.5972, -15.2811, -17.4252, -18.7679, -18.2237, -18.1841, -18.3958,\n        -17.7563, -18.7864, -18.7061, -18.7741, -18.7653, -15.8826, -16.9943,\n        -17.9230, -18.7637, -18.5831, -17.5413, -18.7332, -17.2057, -18.7942,\n        -18.2062, -18.5866, -18.1802, -18.7339, -17.9572, -16.7163, -18.7797,\n        -17.1137, -16.7664, -17.8826, -14.9216,  -4.6717, -17.6114, -14.1898,\n        -18.7993, -18.7304, -12.7856, -18.7754, -15.6328, -18.1375, -17.1017,\n        -18.1405, -17.1576, -14.1854, -15.0755, -15.0010, -18.3973, -18.0807,\n        -18.6380, -17.2482, -18.7762, -18.3060, -17.3303, -15.1132, -17.3226,\n        -18.5681, -14.5456, -13.3135, -17.0056, -17.4386, -17.1593, -18.7971,\n        -18.7916, -16.1990, -18.3689, -15.0683, -11.8793, -18.7670, -18.7221,\n        -18.3848, -18.5506, -16.3985, -16.0136, -18.6964, -18.1679, -12.1138,\n        -18.7017, -18.6430, -18.7566, -17.8686, -14.1663, -18.7457, -17.8425,\n        -18.7944, -18.4616, -16.1451, -18.7897, -18.7603, -18.6971, -18.3480,\n        -15.4398, -18.5632, -18.5416, -17.3719, -16.2296, -18.8072, -18.7924,\n        -13.1963, -18.6588, -18.2064, -17.2842, -13.7105, -18.3414, -16.7689,\n        -13.0601, -18.1636, -18.5989, -18.7566, -18.7398, -18.7975, -17.0262,\n        -17.8593, -18.5498, -18.4562, -18.6777, -18.1768, -18.7103, -18.6488,\n        -18.2757, -16.3172, -17.7772, -17.3573, -18.5400, -18.0823, -14.7362,\n        -14.9259, -14.6622, -18.5415, -18.6304])\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\UOS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning:\n\nImplicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n\n```\n:::\n:::\n\n\n### Ref\n- https://openreview.net/forum?id=BJJLHbb0-\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.11.4\n---\n",
    "supporting": [
      "2023-10-15_files\\figure-ipynb"
    ],
    "filters": []
  }
}