{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **\\[DAGMM\\]** DAGMM: for arrhythmia data set\n",
        "\n",
        "kione kim  \n",
        "2023-10-19\n",
        "\n",
        "## Deep Autoencoding Gaussian Mixture Model for Arrhythmia dataset"
      ],
      "id": "0cad9174-49ec-4342-bada-c6e0a02afae5"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "### imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import sys"
      ],
      "id": "442bcb64"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "### data 파일\n",
        "file_path = 'C:\\\\Users\\\\UOS\\\\Desktop\\\\연구\\\\5. 데이터\\\\data\\\\arrhythmia\\\\arrhythmia.data'\n",
        "\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "df = df.replace('?', 0)\n",
        "df = df.astype('float64')\n",
        "\n",
        "data_array = df.values\n",
        "data_array = torch.autograd.Variable(torch.from_numpy(data_array).float())\n",
        "data_array.shape"
      ],
      "id": "faf89830"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='parser for argparse test')\n",
        "\n",
        "parser.add_argument('--input_dim', type=int, default=data_array.shape[-1])\n",
        "parser.add_argument('--enc_hidden_dim', type=str, default='10,2')\n",
        "parser.add_argument('--dec_hidden_dim', type=str, default='10')\n",
        "parser.add_argument('--est_hidden_dim', type=str, default='4, 10, 2')\n",
        "parser.add_argument('--dropout', action='store_true', default=0.5)\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "parser.add_argument('--num_epoch', type=int, default=10)\n",
        "\n",
        "if 'ipykernel_launcher' in sys.argv[0]:\n",
        "    sys.argv = [sys.argv[0]]  \n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "enc_hidden_dim = args.enc_hidden_dim.split(',')\n",
        "dec_hidden_dim = args.dec_hidden_dim.split(',')\n",
        "est_hidden_dim = args.est_hidden_dim.split(',')\n",
        "\n",
        "args.enc_hidden_dim_list = []\n",
        "args.dec_hidden_dim_list = []\n",
        "args.est_hidden_dim_list = []\n",
        "\n",
        "args.enc_hidden_dim_list.append(args.input_dim)\n",
        "\n",
        "for i in enc_hidden_dim:\n",
        "    args.enc_hidden_dim_list.append(int(i))\n",
        "\n",
        "args.enc_hidden_dim_list\n",
        "\n",
        "args.dec_hidden_dim_list.append(args.enc_hidden_dim_list[-1])\n",
        "\n",
        "for i in dec_hidden_dim:\n",
        "    args.dec_hidden_dim_list.append(int(i))\n",
        "\n",
        "args.dec_hidden_dim_list.append(args.input_dim)\n",
        "\n",
        "args.dec_hidden_dim_list\n",
        "\n",
        "for i in est_hidden_dim:\n",
        "    args.est_hidden_dim_list.append(int(i))\n",
        "\n",
        "args.est_hidden_dim_list\n",
        "\n",
        "args"
      ],
      "id": "cab64970"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "### compresssion network\n",
        "class midlayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(midlayer, self).__init__()\n",
        "        self.fc_layer   = nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = nn.Tanh()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        out = self.fc_layer(input)        \n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim_list):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        layer_list = []\n",
        "        for i in range(len(hidden_dim_list)-2):\n",
        "            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n",
        "        \n",
        "        layer_list.append(nn.Linear(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n",
        "        self.layer = nn.Sequential(*layer_list)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.layer(input)\n",
        "        return out\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim_list):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        layer_list = []\n",
        "        for i in range(len(hidden_dim_list)-2):\n",
        "            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n",
        "        \n",
        "        layer_list.append(midlayer(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n",
        "        self.layer = nn.Sequential(*layer_list)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        out = self.layer(input)\n",
        "        return out\n",
        "\n",
        "class CompressionNet(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(enc_hidden_dim_list)\n",
        "        self.decoder = Decoder(dec_hidden_dim_list)\n",
        "\n",
        "        self._reconstruction_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.encoder(input)\n",
        "        out = self.decoder(out)\n",
        "        return out\n",
        "\n",
        "    def encode(self, input):\n",
        "        return self.encoder(input)\n",
        "\n",
        "    def decode(self, input):\n",
        "        return self.decoder(input)\n",
        "\n",
        "    def reconstuction_loss(self, input, input_target):\n",
        "        target_hat = self(input)\n",
        "        return self._reconstruction_loss(target_hat, input_target)"
      ],
      "id": "ffa59fa8"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "### reconstructed error\n",
        "eps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n",
        "\n",
        "def relative_euclidean_distance(x1, x2, eps=eps):\n",
        "    num = torch.norm(x1 - x2, p=2, dim=1)\n",
        "    denom = torch.norm(x1, p=2, dim=1)\n",
        "    return num / torch.max(denom, eps)\n",
        "\n",
        "def cosine_similarity(x1, x2, eps=eps):\n",
        "    dot_prod = torch.sum(x1 * x2, dim=1)\n",
        "    dist_x1 = torch.norm(x1, p=2, dim=1)\n",
        "    dist_x2 = torch.norm(x2, p=2, dim=1)\n",
        "    return dot_prod / torch.max(dist_x1*dist_x2, eps)"
      ],
      "id": "cbc854b4"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "### estimation network\n",
        "class Estimation(nn.Module):\n",
        "    def __init__(self, est_hidden_dim_list):\n",
        "        super().__init__()\n",
        "        \n",
        "        layer_list = []\n",
        "        for i in range(len(est_hidden_dim_list)-2):\n",
        "            layer_list.append(midlayer(est_hidden_dim_list[i], est_hidden_dim_list[i+1]))\n",
        "        \n",
        "        layer_list.append(nn.Dropout(p=0.5))\n",
        "        layer_list.append(nn.Linear(est_hidden_dim_list[-2], est_hidden_dim_list[-1]))\n",
        "        layer_list.append(nn.Softmax())\n",
        "        self.net = nn.Sequential(*layer_list)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.net(input)\n",
        "        return out"
      ],
      "id": "67df0a6d"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Mixture\n",
        "class Mixture(nn.Module):\n",
        "    def __init__(self, latent_dimension):\n",
        "        super().__init__()\n",
        "        self.latent_dimension = latent_dimension\n",
        "\n",
        "        self.Phi    = np.random.random([1])\n",
        "        self.Phi    = torch.from_numpy(self.Phi).float()\n",
        "        self.Phi    = nn.Parameter(self.Phi, requires_grad = False)\n",
        "\n",
        "        self.mu     = 2.*np.random.random([latent_dimension]) - 0.5\n",
        "        self.mu     = torch.from_numpy(self.mu).float()\n",
        "        self.mu     = nn.Parameter(self.mu, requires_grad = False)\n",
        "\n",
        "        self.Sigma  = np.eye(latent_dimension, latent_dimension)\n",
        "        self.Sigma  = torch.from_numpy(self.Sigma).float()\n",
        "        self.Sigma  = nn.Parameter(self.Sigma, requires_grad = False)\n",
        "        \n",
        "        self.eps_Sigma  = torch.FloatTensor(np.diag([1.e-8 for _ in range(latent_dimension)]))\n",
        "\n",
        "    def forward(self, est_inputs, with_log = True):\n",
        "        batch_size, _   = est_inputs.shape\n",
        "        out_values  = []\n",
        "        inv_sigma   = torch.inverse(self.Sigma)\n",
        "        det_sigma   = np.linalg.det(self.Sigma.data.cpu().numpy())\n",
        "        det_sigma   = torch.from_numpy(det_sigma.reshape([1])).float()\n",
        "        det_sigma   = torch.autograd.Variable(det_sigma)\n",
        "        for est_input in est_inputs:\n",
        "            diff    = (est_input - self.mu).view(-1,1)\n",
        "            out     = -0.5 * torch.mm(torch.mm(diff.view(1,-1), inv_sigma), diff)\n",
        "            out     = (self.Phi * torch.exp(out)) / torch.sqrt(2. * np.pi * det_sigma)\n",
        "            if with_log:\n",
        "                out = -torch.log(out)\n",
        "            out_values.append(float(out.data.cpu().numpy()))\n",
        "\n",
        "        out = torch.autograd.Variable(torch.FloatTensor(out_values))\n",
        "        return out\n",
        "    \n",
        "    def _update_parameters(self, samples, affiliations):\n",
        "        if not self.training:\n",
        "            return\n",
        "\n",
        "        batch_size, _ = samples.shape\n",
        "\n",
        "        # Updating phi.\n",
        "        phi = torch.mean(affiliations)\n",
        "        self.Phi.data = phi.data\n",
        "\n",
        "        # Updating mu.\n",
        "        num = 0.\n",
        "        for i in range(batch_size):\n",
        "            z_i     = samples[i, :]\n",
        "            gamma_i = affiliations[i]\n",
        "            num     += gamma_i * z_i\n",
        "        \n",
        "        denom        = torch.sum(affiliations)\n",
        "        self.mu.data = (num / denom).data\n",
        "\n",
        "        # Updating Sigma.\n",
        "        mu  = self.mu\n",
        "        num = None\n",
        "        for i in range(batch_size):\n",
        "            z_i     = samples[i, :]\n",
        "            gamma_i = affiliations[i]\n",
        "            diff    = (z_i - mu).view(-1, 1)\n",
        "            to_add  = gamma_i * torch.mm(diff, diff.view(1, -1))\n",
        "            if num is None:\n",
        "                num = to_add\n",
        "            else:\n",
        "                num += to_add\n",
        "\n",
        "        denom           = torch.sum(affiliations)\n",
        "        self.Sigma.data = (num / denom).data + self.eps_Sigma\n",
        "\n",
        "\n",
        "class GMM(nn.Module):\n",
        "    def __init__(self, num_mixtures, latent_dimension):\n",
        "        super().__init__()\n",
        "        self.num_mixtures       = num_mixtures\n",
        "        self.latent_dimension   = latent_dimension\n",
        "\n",
        "        mixtures        = [Mixture(latent_dimension) for _ in range(num_mixtures)]\n",
        "        self.mixtures   = nn.ModuleList(mixtures)\n",
        "    \n",
        "    def forward(self, est_inputs):\n",
        "        out = None\n",
        "        for mixture in self.mixtures:\n",
        "            to_add  = mixture(est_inputs, with_log = False)\n",
        "            if out is None:\n",
        "                out = to_add\n",
        "            else:\n",
        "                out += to_add\n",
        "        return -torch.log(out)\n",
        "    \n",
        "    def _update_mixtures_parameters(self, samples, mixtures_affiliations):\n",
        "        if not self.training:\n",
        "            return\n",
        "\n",
        "        for i, mixture in enumerate(self.mixtures):\n",
        "            affiliations = mixtures_affiliations[:, i]\n",
        "            mixture._update_parameters(samples, affiliations)"
      ],
      "id": "f01ee752"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "### model\n",
        "class DAGMM(nn.Module):\n",
        "    def __init__(self, compression_module, estimation_module, gmm_module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.compressor = compression_module\n",
        "        self.estimator  = estimation_module\n",
        "        self.gmm        = gmm_module\n",
        "\n",
        "    def forward(self, input):\n",
        "        encoded = self.compressor.encode(input)\n",
        "        decoded = self.compressor.decode(encoded)\n",
        "\n",
        "        relative_ed     = relative_euclidean_distance(input, decoded)\n",
        "        cosine_sim      = cosine_similarity(input, decoded)\n",
        "\n",
        "        relative_ed     = relative_ed.view(-1, 1)\n",
        "        cosine_sim      = relative_ed.view(-1, 1)\n",
        "        latent_vectors  = torch.cat([encoded, relative_ed, cosine_sim], dim=1)\n",
        "\n",
        "        if self.training:\n",
        "            mixtures_affiliations = self.estimator(latent_vectors)\n",
        "            self.gmm._update_mixtures_parameters(latent_vectors,\n",
        "                                                 mixtures_affiliations)\n",
        "        return self.gmm(latent_vectors)\n",
        "\n",
        "\n",
        "class DAGMMArrhythmia(DAGMM):\n",
        "    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list, est_hidden_dim_list):\n",
        "        compressor  = CompressionNet(enc_hidden_dim_list, dec_hidden_dim_list)\n",
        "        estimator   = Estimation(est_hidden_dim_list)\n",
        "        gmm = GMM(num_mixtures=2, latent_dimension=4)\n",
        "\n",
        "        super().__init__(compression_module = compressor,\n",
        "                         estimation_module  = estimator,\n",
        "                         gmm_module         = gmm)"
      ],
      "id": "84a8de0d"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "### tests\n",
        "def test_dagmm():\n",
        "    net = DAGMMArrhythmia(args.enc_hidden_dim_list, args.dec_hidden_dim_list, args.est_hidden_dim_list)\n",
        "    out = net(data_array)\n",
        "    print(out)\n",
        "\n",
        "def convert_to_var(input):\n",
        "    out = torch.from_numpy(input).float()\n",
        "    out = torch.autograd.Variable(out)\n",
        "    return out\n",
        "\n",
        "def test_update_mixture():\n",
        "    batch_size       = 5\n",
        "    latent_dimension = 7\n",
        "    mix              = Mixture(latent_dimension)\n",
        "    latent_vectors   = np.random.random([batch_size, latent_dimension])\n",
        "    affiliations     = np.random.random([batch_size])\n",
        "    latent_vectors   = convert_to_var(latent_vectors)\n",
        "    affiliations     = convert_to_var(affiliations)\n",
        "\n",
        "    for param in mix.parameters():\n",
        "        print(param)\n",
        "\n",
        "    mix.train()\n",
        "    mix._update_parameters(latent_vectors, affiliations)\n",
        "\n",
        "    for param in mix.parameters():\n",
        "        print(param)\n",
        "\n",
        "\n",
        "def test_forward_mixture():\n",
        "    batch_size       = 5\n",
        "    latent_dimension = 7\n",
        "\n",
        "    mix = Mixture(latent_dimension)\n",
        "    latent_vectors   = np.random.random([batch_size, latent_dimension])\n",
        "    latent_vectors   = convert_to_var(latent_vectors)\n",
        "\n",
        "    mix.train()\n",
        "    out = mix(latent_vectors)\n",
        "    print(out)\n",
        "\n",
        "\n",
        "def test_update_gmm():\n",
        "    batch_size      = int(5)\n",
        "    latent_dimension= 7\n",
        "    num_mixtures    = 2\n",
        "\n",
        "    gmm = GMM(num_mixtures, latent_dimension)\n",
        "\n",
        "    latent_vectors  = np.random.random([batch_size, latent_dimension])\n",
        "    latent_vectors  = convert_to_var(latent_vectors)\n",
        "\n",
        "    affiliations    = np.random.random([batch_size, num_mixtures])\n",
        "    affiliations    = convert_to_var(affiliations)\n",
        "\n",
        "    for param in gmm.parameters():\n",
        "        print(param)\n",
        "\n",
        "    gmm.train()\n",
        "    gmm._update_mixtures_parameters(latent_vectors, affiliations)\n",
        "\n",
        "    for param in gmm.parameters():\n",
        "        print(param)"
      ],
      "id": "84773bc7"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([0.9789])\n",
            "Parameter containing:\n",
            "tensor([ 0.9942,  1.4710,  0.4338,  0.6034, -0.4222,  1.3985,  1.0799])\n",
            "Parameter containing:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1.]])\n",
            "Parameter containing:\n",
            "tensor(0.3604)\n",
            "Parameter containing:\n",
            "tensor([0.4632, 0.4196, 0.1300, 0.3359, 0.4307, 0.6684, 0.3321])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0693,  0.0277, -0.0179,  0.0197,  0.0092,  0.0369, -0.0007],\n",
            "        [ 0.0277,  0.0322, -0.0114, -0.0041, -0.0192,  0.0160, -0.0111],\n",
            "        [-0.0179, -0.0114,  0.0064, -0.0014,  0.0040, -0.0102,  0.0033],\n",
            "        [ 0.0197, -0.0041, -0.0014,  0.0146,  0.0197,  0.0118,  0.0110],\n",
            "        [ 0.0092, -0.0192,  0.0040,  0.0197,  0.0350,  0.0110,  0.0255],\n",
            "        [ 0.0369,  0.0160, -0.0102,  0.0118,  0.0110,  0.0322,  0.0165],\n",
            "        [-0.0007, -0.0111,  0.0033,  0.0110,  0.0255,  0.0165,  0.0326]])\n",
            "tensor([4.0933, 3.0756, 3.4852, 3.7671, 3.0913])\n",
            "Parameter containing:\n",
            "tensor([0.1730])\n",
            "Parameter containing:\n",
            "tensor([-0.2267,  0.1143,  0.6347,  0.9368,  1.3687,  1.4179,  1.4022])\n",
            "Parameter containing:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1.]])\n",
            "Parameter containing:\n",
            "tensor([0.1261])\n",
            "Parameter containing:\n",
            "tensor([0.1376, 1.0249, 0.8905, 1.2541, 0.7198, 0.1575, 0.3876])\n",
            "Parameter containing:\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1.]])\n",
            "Parameter containing:\n",
            "tensor(0.4438)\n",
            "Parameter containing:\n",
            "tensor([0.2877, 0.6446, 0.3411, 0.8348, 0.3113, 0.3858, 0.3688])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0901, -0.0324,  0.0927,  0.0119,  0.0457,  0.0479, -0.0029],\n",
            "        [-0.0324,  0.0534, -0.0168, -0.0116, -0.0162, -0.0613,  0.0065],\n",
            "        [ 0.0927, -0.0168,  0.1116,  0.0145,  0.0414,  0.0207, -0.0125],\n",
            "        [ 0.0119, -0.0116,  0.0145,  0.0061,  0.0032,  0.0088, -0.0073],\n",
            "        [ 0.0457, -0.0162,  0.0414,  0.0032,  0.0267,  0.0310,  0.0055],\n",
            "        [ 0.0479, -0.0613,  0.0207,  0.0088,  0.0310,  0.0857,  0.0065],\n",
            "        [-0.0029,  0.0065, -0.0125, -0.0073,  0.0055,  0.0065,  0.0150]])\n",
            "Parameter containing:\n",
            "tensor(0.4524)\n",
            "Parameter containing:\n",
            "tensor([0.5331, 0.5054, 0.4609, 0.6493, 0.4372, 0.4965, 0.4290])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0888,  0.0680,  0.0634, -0.1060,  0.0604, -0.0590,  0.0679],\n",
            "        [ 0.0680,  0.1318,  0.0816, -0.1095,  0.0398, -0.1497,  0.0548],\n",
            "        [ 0.0634,  0.0816,  0.0675, -0.0738,  0.0390, -0.0841,  0.0428],\n",
            "        [-0.1060, -0.1095, -0.0738,  0.1597, -0.0718,  0.1108, -0.0930],\n",
            "        [ 0.0604,  0.0398,  0.0390, -0.0718,  0.0420, -0.0313,  0.0473],\n",
            "        [-0.0590, -0.1497, -0.0841,  0.1108, -0.0313,  0.1779, -0.0495],\n",
            "        [ 0.0679,  0.0548,  0.0428, -0.0930,  0.0473, -0.0495,  0.0578]])\n",
            "tensor([-20.3789, -20.4527,  -4.2941, -20.7020, -20.4688, -14.3540, -20.6867,\n",
            "        -20.5999, -20.6242, -20.5650, -15.6302, -20.6579, -19.0589, -19.5618,\n",
            "        -20.6168, -20.3740, -14.2925, -19.4922, -20.7009, -20.7164, -18.7185,\n",
            "        -20.2958, -12.0397, -20.5567, -20.5974, -20.7132, -19.0564, -20.4641,\n",
            "        -16.4131, -15.5303, -17.5047, -19.8949, -20.5945, -20.6701, -20.5027,\n",
            "        -20.4058, -20.7177, -20.5814, -20.5591, -19.4831, -20.6349, -15.7905,\n",
            "        -18.7755, -20.7177, -19.4295, -20.0287, -20.1642, -20.6310, -20.5931,\n",
            "        -20.5688, -20.5797, -20.5141, -20.1121, -19.7199, -18.9550, -20.6948,\n",
            "        -20.7088, -19.1649, -20.7118, -20.5822,  -2.9701, -19.9853, -20.4913,\n",
            "        -20.5251, -20.6722, -20.6885, -20.7168, -19.5477, -20.5119, -20.7100,\n",
            "        -20.6845, -19.0991, -20.6299, -20.2166, -20.6264, -20.7094, -20.0583,\n",
            "        -20.6911, -16.4325, -19.0133, -19.1678, -20.1747, -20.3389, -20.4737,\n",
            "        -20.6282, -12.7427, -20.6241, -19.7058, -19.0780, -17.3551, -20.2275,\n",
            "        -20.2535, -20.4328, -19.6070, -20.1104, -19.0387, -19.0081, -20.6750,\n",
            "        -19.4763, -20.4738, -20.6885, -20.3298, -19.1182, -20.5858, -20.7127,\n",
            "        -20.6744, -20.4495, -20.6538, -20.7118, -20.7132, -19.0893, -20.3693,\n",
            "        -20.6216, -18.2459, -20.6037, -20.7152, -19.3004, -20.6774, -20.1759,\n",
            "        -20.6729, -20.7010, -20.7132, -20.6332, -20.6003, -20.7043, -18.9932,\n",
            "        -20.2079, -19.6506, -20.5835, -20.7120, -19.9610, -20.6997, -20.6700,\n",
            "        -18.8883, -20.3085, -14.7531, -20.7118, -20.7012, -15.5035, -20.5927,\n",
            "        -19.8779, -17.3776, -20.6828, -18.9815, -20.5542, -20.7134, -20.6496,\n",
            "        -20.5508, -20.1591, -20.4493, -20.6581, -20.6719, -19.5918, -20.5872,\n",
            "        -20.6641, -20.5660, -20.2578, -15.4465, -20.6891, -20.6109, -20.5004,\n",
            "        -20.6759, -20.2219, -20.6806, -20.2596, -20.7177, -20.6784, -20.6998,\n",
            "        -20.5336, -20.6720, -19.6264, -20.7123, -19.5796, -18.6583, -20.1704,\n",
            "        -18.2439, -19.9228, -18.8291, -20.6543, -20.4454, -20.5885, -20.6824,\n",
            "        -20.2389, -20.5084, -19.0204, -20.5659, -20.1417,  -7.1837, -20.7126,\n",
            "        -20.3340, -15.9161, -20.3640, -20.7139, -20.2361, -20.2304, -20.6972,\n",
            "        -15.2451, -20.7088, -18.9664, -20.5527, -20.4333, -20.1413, -20.5917,\n",
            "        -20.5085, -20.3821, -15.6011, -20.6753, -20.5174, -20.3588, -20.7176,\n",
            "        -15.2038, -15.7339, -20.2332, -20.7080, -19.8842, -20.5800,  -7.0088,\n",
            "        -18.1443, -19.3164, -20.5322, -20.6506, -20.6862, -20.3429, -20.1482,\n",
            "        -17.9157, -20.2289, -19.6615, -20.7170, -19.6146,  -7.7819, -20.3843,\n",
            "        -19.7689, -20.6404, -19.5936, -18.4620, -18.7437, -20.3182, -20.7004,\n",
            "        -20.6328, -18.8078, -20.5371, -20.1067, -20.7140, -16.2822, -19.5844,\n",
            "        -19.8715, -20.7139, -19.2248, -20.3656, -19.9499, -20.5709, -20.0903,\n",
            "        -18.3902, -16.1828, -20.7179, -20.5636, -20.1756, -20.2311, -20.2361,\n",
            "        -19.5084, -20.5797, -20.4558, -20.6564, -20.6043, -20.1388, -20.3674,\n",
            "        -15.5062, -19.8285, -20.6748, -20.6977, -20.0449, -18.8170, -20.1797,\n",
            "        -20.7174, -20.6149, -20.6858, -20.6790, -18.4600, -20.3371, -20.3612,\n",
            "        -20.3021, -20.6942, -20.4800, -17.9644, -20.1009, -20.5843, -20.5938,\n",
            "        -20.0971, -20.7180, -20.0184, -20.4943, -20.7090, -20.0200, -18.8574,\n",
            "        -20.6164, -17.7504, -20.6528,  -4.5388, -20.6045, -20.5640, -20.4637,\n",
            "        -20.6638, -20.1560, -20.6651, -20.7007, -20.4794, -20.6430, -19.9731,\n",
            "        -20.0321, -20.2451, -20.4637, -20.5777, -15.5212, -15.6686, -20.7046,\n",
            "        -20.6217, -18.4426, -20.6707, -18.6353, -15.5195, -16.4041, -20.4709,\n",
            "        -18.8213, -16.4347, -20.7077, -20.6917, -20.6765, -20.7030, -20.3301,\n",
            "        -20.7156, -20.6205, -18.9673, -16.9882, -20.6346, -20.4414, -20.4284,\n",
            "        -20.6086, -19.9608, -20.5516, -20.6805, -15.3103, -20.6353, -20.6576,\n",
            "        -20.6786, -20.6986, -20.7168, -20.7089, -20.6797, -15.4024, -19.5317,\n",
            "        -20.1311, -16.2073, -20.5583, -19.7229, -20.3382, -15.1684, -19.6027,\n",
            "        -20.6479, -20.5251, -18.2144, -20.3687, -20.5147, -20.7173, -15.6632,\n",
            "        -20.2460, -20.3609,  -8.1868, -20.7137, -20.7164, -20.6780, -20.6334,\n",
            "        -18.9398, -20.7086, -20.2352, -20.5499, -19.9749, -20.0366, -20.6006,\n",
            "        -20.3190,  -4.5846, -11.2436, -16.0929, -20.6080, -20.5588, -14.9094,\n",
            "        -20.4521, -20.0049, -17.1061, -15.3424, -19.9881, -20.5174, -19.2034,\n",
            "        -20.5309, -20.2696, -11.8284, -19.0942, -17.6486,  -5.9191, -20.7179,\n",
            "        -20.6766, -20.5197, -18.4740, -20.7179,  -6.0091, -20.5677, -20.1780,\n",
            "        -20.4069, -20.1223,  -4.7818, -19.5721, -19.0524, -19.8197, -18.8549,\n",
            "        -19.8507, -19.3223, -18.7649, -20.7180, -15.3776, -20.5123, -20.5812,\n",
            "        -20.5972, -20.5708, -20.2171, -20.3693, -14.2864, -20.3662, -20.7067,\n",
            "        -20.0895, -20.7093, -17.1434, -19.9378, -20.6493, -19.5653, -19.5809,\n",
            "        -20.7172, -20.5874, -19.9009, -17.6919, -20.4855, -16.1358, -20.4774,\n",
            "        -19.8548, -20.6725, -19.3450, -20.6923, -20.6266, -20.5260, -20.5583,\n",
            "        -20.3138, -13.7640, -20.7085, -20.3325])"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\UOS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning:\n",
            "\n",
            "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    test_update_mixture()\n",
        "    test_forward_mixture()\n",
        "    test_update_gmm()\n",
        "    test_dagmm()"
      ],
      "id": "2ba94542"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ref\n",
        "\n",
        "-   https://openreview.net/forum?id=BJJLHbb0-"
      ],
      "id": "9782d226-2ae1-475b-90b0-76eb70191793"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  }
}