[
  {
    "objectID": "posts/DAGMM/index.html",
    "href": "posts/DAGMM/index.html",
    "title": "DAGMM",
    "section": "",
    "text": "This is a post"
  },
  {
    "objectID": "posts/Autoencoder/index.html",
    "href": "posts/Autoencoder/index.html",
    "title": "Autoencoder",
    "section": "",
    "text": "This in first contents"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog was created for my personal research, study.\n\nTip\n\npreview: To render and preview, execute the Quarto: Render command. You can alternatively use the Ctrl+Shift+K keyboard shortcut, or the Render button at the top right of the editor"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "irumae",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 19, 2023\n\n\n[DAGMM] DAGMM: for arrhythmia data set\n\n\nkione kim\n\n\n\n\nOct 16, 2023\n\n\nDAGMM\n\n\nkione kim\n\n\n\n\nOct 10, 2023\n\n\nAutoencoder\n\n\nkione kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DAGMM/2023-10-15.html",
    "href": "posts/DAGMM/2023-10-15.html",
    "title": "[DAGMM] DAGMM: for arrhythmia data set",
    "section": "",
    "text": "### imports\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport sys\n\n\n### data 파일\nfile_path = 'C:\\\\Users\\\\UOS\\\\Desktop\\\\연구\\\\5. 데이터\\\\data\\\\arrhythmia\\\\arrhythmia.data'\n\ndf = pd.read_csv(file_path, header=None)\ndf = df.replace('?', 0)\ndf = df.astype('float64')\n\ndata_array = df.values\ndata_array = torch.autograd.Variable(torch.from_numpy(data_array).float())\ndata_array.shape\n\ntorch.Size([452, 280])\n\n\n\nparser = argparse.ArgumentParser(description='parser for argparse test')\n\nparser.add_argument('--input_dim', type=int, default=data_array.shape[-1])\nparser.add_argument('--enc_hidden_dim', type=str, default='10,2')\nparser.add_argument('--dec_hidden_dim', type=str, default='10')\nparser.add_argument('--est_hidden_dim', type=str, default='4, 10, 2')\nparser.add_argument('--dropout', action='store_true', default=0.5)\nparser.add_argument('--learning_rate', type=float, default=0.001)\nparser.add_argument('--num_epoch', type=int, default=10)\n\nif 'ipykernel_launcher' in sys.argv[0]:\n    sys.argv = [sys.argv[0]]  \n\nargs = parser.parse_args()\n\nenc_hidden_dim = args.enc_hidden_dim.split(',')\ndec_hidden_dim = args.dec_hidden_dim.split(',')\nest_hidden_dim = args.est_hidden_dim.split(',')\n\nargs.enc_hidden_dim_list = []\nargs.dec_hidden_dim_list = []\nargs.est_hidden_dim_list = []\n\nargs.enc_hidden_dim_list.append(args.input_dim)\n\nfor i in enc_hidden_dim:\n    args.enc_hidden_dim_list.append(int(i))\n\nargs.enc_hidden_dim_list\n\nargs.dec_hidden_dim_list.append(args.enc_hidden_dim_list[-1])\n\nfor i in dec_hidden_dim:\n    args.dec_hidden_dim_list.append(int(i))\n\nargs.dec_hidden_dim_list.append(args.input_dim)\n\nargs.dec_hidden_dim_list\n\nfor i in est_hidden_dim:\n    args.est_hidden_dim_list.append(int(i))\n\nargs.est_hidden_dim_list\n\nargs\n\nNamespace(input_dim=280, enc_hidden_dim='10,2', dec_hidden_dim='10', est_hidden_dim='4, 10, 2', dropout=0.5, learning_rate=0.001, num_epoch=10, enc_hidden_dim_list=[280, 10, 2], dec_hidden_dim_list=[2, 10, 280], est_hidden_dim_list=[4, 10, 2])\n\n\n\n### compresssion network\nclass midlayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(midlayer, self).__init__()\n        self.fc_layer   = nn.Linear(input_dim, hidden_dim)\n        self.activation = nn.Tanh()\n    \n    def forward(self, input):\n        out = self.fc_layer(input)        \n        out = self.activation(out)\n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Encoder, self).__init__()\n        \n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Linear(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n\n    def forward(self, input):\n        out = self.layer(input)\n        return out\n    \nclass Decoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Decoder, self).__init__()\n\n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(midlayer(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n    \n    def forward(self, input):\n        out = self.layer(input)\n        return out\n\nclass CompressionNet(nn.Module):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list):\n        super().__init__()\n        self.encoder = Encoder(enc_hidden_dim_list)\n        self.decoder = Decoder(dec_hidden_dim_list)\n\n        self._reconstruction_loss = nn.MSELoss()\n\n    def forward(self, input):\n        out = self.encoder(input)\n        out = self.decoder(out)\n        return out\n\n    def encode(self, input):\n        return self.encoder(input)\n\n    def decode(self, input):\n        return self.decoder(input)\n\n    def reconstuction_loss(self, input, input_target):\n        target_hat = self(input)\n        return self._reconstruction_loss(target_hat, input_target)\n\n\n### reconstructed error\neps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n\ndef relative_euclidean_distance(x1, x2, eps=eps):\n    num = torch.norm(x1 - x2, p=2, dim=1)\n    denom = torch.norm(x1, p=2, dim=1)\n    return num / torch.max(denom, eps)\n\ndef cosine_similarity(x1, x2, eps=eps):\n    dot_prod = torch.sum(x1 * x2, dim=1)\n    dist_x1 = torch.norm(x1, p=2, dim=1)\n    dist_x2 = torch.norm(x2, p=2, dim=1)\n    return dot_prod / torch.max(dist_x1*dist_x2, eps)\n\n\n### estimation network\nclass Estimation(nn.Module):\n    def __init__(self, est_hidden_dim_list):\n        super().__init__()\n        \n        layer_list = []\n        for i in range(len(est_hidden_dim_list)-2):\n            layer_list.append(midlayer(est_hidden_dim_list[i], est_hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Dropout(p=0.5))\n        layer_list.append(nn.Linear(est_hidden_dim_list[-2], est_hidden_dim_list[-1]))\n        layer_list.append(nn.Softmax())\n        self.net = nn.Sequential(*layer_list)\n        \n    def forward(self, input):\n        out = self.net(input)\n        return out\n\n\n### Mixture\nclass Mixture(nn.Module):\n    def __init__(self, latent_dimension):\n        super().__init__()\n        self.latent_dimension = latent_dimension\n\n        self.Phi    = np.random.random([1])\n        self.Phi    = torch.from_numpy(self.Phi).float()\n        self.Phi    = nn.Parameter(self.Phi, requires_grad = False)\n\n        self.mu     = 2.*np.random.random([latent_dimension]) - 0.5\n        self.mu     = torch.from_numpy(self.mu).float()\n        self.mu     = nn.Parameter(self.mu, requires_grad = False)\n\n        self.Sigma  = np.eye(latent_dimension, latent_dimension)\n        self.Sigma  = torch.from_numpy(self.Sigma).float()\n        self.Sigma  = nn.Parameter(self.Sigma, requires_grad = False)\n        \n        self.eps_Sigma  = torch.FloatTensor(np.diag([1.e-8 for _ in range(latent_dimension)]))\n\n    def forward(self, est_inputs, with_log = True):\n        batch_size, _   = est_inputs.shape\n        out_values  = []\n        inv_sigma   = torch.inverse(self.Sigma)\n        det_sigma   = np.linalg.det(self.Sigma.data.cpu().numpy())\n        det_sigma   = torch.from_numpy(det_sigma.reshape([1])).float()\n        det_sigma   = torch.autograd.Variable(det_sigma)\n        for est_input in est_inputs:\n            diff    = (est_input - self.mu).view(-1,1)\n            out     = -0.5 * torch.mm(torch.mm(diff.view(1,-1), inv_sigma), diff)\n            out     = (self.Phi * torch.exp(out)) / torch.sqrt(2. * np.pi * det_sigma)\n            if with_log:\n                out = -torch.log(out)\n            out_values.append(float(out.data.cpu().numpy()))\n\n        out = torch.autograd.Variable(torch.FloatTensor(out_values))\n        return out\n    \n    def _update_parameters(self, samples, affiliations):\n        if not self.training:\n            return\n\n        batch_size, _ = samples.shape\n\n        # Updating phi.\n        phi = torch.mean(affiliations)\n        self.Phi.data = phi.data\n\n        # Updating mu.\n        num = 0.\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            num     += gamma_i * z_i\n        \n        denom        = torch.sum(affiliations)\n        self.mu.data = (num / denom).data\n\n        # Updating Sigma.\n        mu  = self.mu\n        num = None\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            diff    = (z_i - mu).view(-1, 1)\n            to_add  = gamma_i * torch.mm(diff, diff.view(1, -1))\n            if num is None:\n                num = to_add\n            else:\n                num += to_add\n\n        denom           = torch.sum(affiliations)\n        self.Sigma.data = (num / denom).data + self.eps_Sigma\n\n\nclass GMM(nn.Module):\n    def __init__(self, num_mixtures, latent_dimension):\n        super().__init__()\n        self.num_mixtures       = num_mixtures\n        self.latent_dimension   = latent_dimension\n\n        mixtures        = [Mixture(latent_dimension) for _ in range(num_mixtures)]\n        self.mixtures   = nn.ModuleList(mixtures)\n    \n    def forward(self, est_inputs):\n        out = None\n        for mixture in self.mixtures:\n            to_add  = mixture(est_inputs, with_log = False)\n            if out is None:\n                out = to_add\n            else:\n                out += to_add\n        return -torch.log(out)\n    \n    def _update_mixtures_parameters(self, samples, mixtures_affiliations):\n        if not self.training:\n            return\n\n        for i, mixture in enumerate(self.mixtures):\n            affiliations = mixtures_affiliations[:, i]\n            mixture._update_parameters(samples, affiliations)\n\n\n### model\nclass DAGMM(nn.Module):\n    def __init__(self, compression_module, estimation_module, gmm_module):\n        super().__init__()\n\n        self.compressor = compression_module\n        self.estimator  = estimation_module\n        self.gmm        = gmm_module\n\n    def forward(self, input):\n        encoded = self.compressor.encode(input)\n        decoded = self.compressor.decode(encoded)\n\n        relative_ed     = relative_euclidean_distance(input, decoded)\n        cosine_sim      = cosine_similarity(input, decoded)\n\n        relative_ed     = relative_ed.view(-1, 1)\n        cosine_sim      = relative_ed.view(-1, 1)\n        latent_vectors  = torch.cat([encoded, relative_ed, cosine_sim], dim=1)\n\n        if self.training:\n            mixtures_affiliations = self.estimator(latent_vectors)\n            self.gmm._update_mixtures_parameters(latent_vectors,\n                                                 mixtures_affiliations)\n        return self.gmm(latent_vectors)\n\n\nclass DAGMMArrhythmia(DAGMM):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list, est_hidden_dim_list):\n        compressor  = CompressionNet(enc_hidden_dim_list, dec_hidden_dim_list)\n        estimator   = Estimation(est_hidden_dim_list)\n        gmm = GMM(num_mixtures=2, latent_dimension=4)\n\n        super().__init__(compression_module = compressor,\n                         estimation_module  = estimator,\n                         gmm_module         = gmm)\n\n\n### tests\ndef test_dagmm():\n    net = DAGMMArrhythmia(args.enc_hidden_dim_list, args.dec_hidden_dim_list, args.est_hidden_dim_list)\n    out = net(data_array)\n    print(out)\n\ndef convert_to_var(input):\n    out = torch.from_numpy(input).float()\n    out = torch.autograd.Variable(out)\n    return out\n\ndef test_update_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n    mix              = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    affiliations     = np.random.random([batch_size])\n    latent_vectors   = convert_to_var(latent_vectors)\n    affiliations     = convert_to_var(affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n    mix.train()\n    mix._update_parameters(latent_vectors, affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n\ndef test_forward_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n\n    mix = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    latent_vectors   = convert_to_var(latent_vectors)\n\n    mix.train()\n    out = mix(latent_vectors)\n    print(out)\n\n\ndef test_update_gmm():\n    batch_size      = int(5)\n    latent_dimension= 7\n    num_mixtures    = 2\n\n    gmm = GMM(num_mixtures, latent_dimension)\n\n    latent_vectors  = np.random.random([batch_size, latent_dimension])\n    latent_vectors  = convert_to_var(latent_vectors)\n\n    affiliations    = np.random.random([batch_size, num_mixtures])\n    affiliations    = convert_to_var(affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n    gmm.train()\n    gmm._update_mixtures_parameters(latent_vectors, affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n\nif __name__ == '__main__':\n    test_update_mixture()\n    test_forward_mixture()\n    test_update_gmm()\n    test_dagmm()\n\nParameter containing:\ntensor([0.1173])\nParameter containing:\ntensor([ 1.1475, -0.3912, -0.4815,  0.8666,  1.2454, -0.2445,  1.4350])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.4673)\nParameter containing:\ntensor([0.5283, 0.2903, 0.5008, 0.5089, 0.3621, 0.4749, 0.4967])\nParameter containing:\ntensor([[ 0.0439,  0.0255, -0.0151,  0.0544,  0.0647,  0.0090,  0.0726],\n        [ 0.0255,  0.0419, -0.0004,  0.0360,  0.0314,  0.0235,  0.0499],\n        [-0.0151, -0.0004,  0.0240,  0.0005, -0.0159,  0.0520, -0.0083],\n        [ 0.0544,  0.0360,  0.0005,  0.0954,  0.0900,  0.0640,  0.1063],\n        [ 0.0647,  0.0314, -0.0159,  0.0900,  0.1013,  0.0334,  0.1124],\n        [ 0.0090,  0.0235,  0.0520,  0.0640,  0.0334,  0.1681,  0.0645],\n        [ 0.0726,  0.0499, -0.0083,  0.1063,  0.1124,  0.0645,  0.1351]])\ntensor([4.8554, 4.6761, 4.4661, 4.8710, 4.8982])\nParameter containing:\ntensor([0.9724])\nParameter containing:\ntensor([ 0.3688,  0.9626,  0.7468, -0.3848,  0.9530,  0.7081,  0.9869])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor([0.3145])\nParameter containing:\ntensor([1.4138, 0.4490, 1.3144, 0.2030, 1.1138, 0.8565, 0.0193])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.6101)\nParameter containing:\ntensor([0.2734, 0.7025, 0.5866, 0.5380, 0.4088, 0.3543, 0.5669])\nParameter containing:\ntensor([[ 0.0827, -0.0232, -0.0216, -0.0052, -0.0009,  0.0318, -0.0217],\n        [-0.0232,  0.0179,  0.0211,  0.0308,  0.0019, -0.0294,  0.0174],\n        [-0.0216,  0.0211,  0.0355,  0.0366, -0.0125, -0.0311,  0.0099],\n        [-0.0052,  0.0308,  0.0366,  0.0781,  0.0157, -0.0543,  0.0373],\n        [-0.0009,  0.0019, -0.0125,  0.0157,  0.0748,  0.0095,  0.0413],\n        [ 0.0318, -0.0294, -0.0311, -0.0543,  0.0095,  0.0582, -0.0249],\n        [-0.0217,  0.0174,  0.0099,  0.0373,  0.0413, -0.0249,  0.0391]])\nParameter containing:\ntensor(0.7920)\nParameter containing:\ntensor([0.3664, 0.7017, 0.5854, 0.6048, 0.4577, 0.3511, 0.5929])\nParameter containing:\ntensor([[ 0.1143, -0.0355, -0.0402, -0.0143,  0.0052,  0.0471, -0.0279],\n        [-0.0355,  0.0251,  0.0335,  0.0389, -0.0113, -0.0418,  0.0165],\n        [-0.0402,  0.0335,  0.0528,  0.0540, -0.0270, -0.0540,  0.0137],\n        [-0.0143,  0.0389,  0.0540,  0.0882, -0.0128, -0.0710,  0.0290],\n        [ 0.0052, -0.0113, -0.0270, -0.0128,  0.0901,  0.0411,  0.0349],\n        [ 0.0471, -0.0418, -0.0540, -0.0710,  0.0411,  0.0829, -0.0191],\n        [-0.0279,  0.0165,  0.0137,  0.0290,  0.0349, -0.0191,  0.0330]])\ntensor([-16.9612, -17.8355, -17.1198, -17.8214, -16.9765, -13.4832, -17.8883,\n        -16.8427, -16.7266, -17.4553, -16.4203, -16.9409, -17.6146, -15.1495,\n        -15.9379, -14.9479, -16.1854, -17.5789, -16.6280, -16.3914, -17.8705,\n        -16.4540, -17.6894, -17.6952, -17.9105, -16.9977, -16.9534, -17.4336,\n        -17.0536, -16.7684, -16.9417, -16.7795, -17.7103, -16.8721, -16.0556,\n        -16.6950, -17.5597, -17.4694, -17.2260, -16.6612, -17.3744, -16.9852,\n        -15.9520, -15.9058, -16.7894, -16.7476, -16.5294, -17.4851, -17.1710,\n        -17.6680, -17.7744, -17.4803, -16.1885, -14.6914, -17.6060, -17.8666,\n        -15.9840, -16.8587, -17.0594, -15.2725, -12.6417, -16.6181, -16.4920,\n        -16.8071, -17.3286, -17.1299, -17.0695, -15.1893, -16.3951, -16.9207,\n        -17.8430, -17.5642, -17.4121, -17.1715, -16.3110, -16.8747,  -9.4447,\n        -17.1424, -16.8413, -15.5074, -16.9420, -16.5711, -16.9047, -17.0329,\n        -16.1576, -12.1645, -15.3441, -17.2765, -13.0270, -17.2411, -16.8029,\n        -17.0612, -16.9580, -16.4213, -16.3675, -17.3942, -16.8937, -16.7398,\n        -16.9648, -14.1587, -17.4364, -15.2745, -16.3091, -16.2499, -16.8516,\n        -17.1280, -13.5346, -17.6581, -13.9843, -17.1864, -16.6246, -16.5804,\n        -16.8307, -15.3374, -17.8115, -17.0566, -16.4458, -15.4481, -17.3106,\n        -16.7912, -17.0978, -16.9524, -16.8946, -16.9953, -16.8141, -16.8253,\n        -17.8658, -15.8182, -17.7361, -17.1825, -17.2128, -17.8637, -15.8899,\n        -13.8260, -17.1471, -16.3404, -17.5933, -16.9136, -17.3709, -17.0664,\n        -16.5006, -12.6482, -16.4738, -16.5775, -16.7379, -17.5284, -16.8920,\n        -17.3995, -17.0317, -17.3837, -17.0141, -15.2970, -16.1848, -16.9914,\n        -17.8913, -17.4832, -17.8995, -16.2905, -15.0831, -16.9196, -17.1523,\n        -16.7555, -17.1981, -15.8082, -16.2761, -17.2934, -16.5064, -16.0278,\n        -17.3910, -17.2586, -15.3088, -16.0010, -17.4784, -17.1952, -16.1224,\n        -17.5613, -16.2742, -17.6207, -16.7494, -16.2702, -17.0538, -16.6950,\n        -17.7900, -17.1539, -15.8621, -16.9515, -16.4764, -16.7403, -15.7678,\n        -15.0590, -17.3892, -16.1987, -14.6992, -16.0648, -17.3860, -17.0080,\n        -17.8890, -16.5353, -15.8924, -16.6831, -17.8159, -17.6439, -16.6141,\n        -17.5179, -11.3079, -14.3721, -16.9241, -11.8753, -12.8442, -17.4653,\n        -17.2166, -16.8058, -17.2556, -15.4820, -17.3901, -16.7357, -16.1637,\n        -12.8449, -14.0127, -17.8357, -17.0824, -16.0741, -16.9676, -17.7164,\n        -16.6101, -15.9101, -16.6739, -16.7744, -15.0311, -16.1429, -17.6085,\n        -17.2562, -17.5409, -16.1831, -15.0784, -15.8370, -17.1719, -16.1404,\n        -17.5857, -17.6243, -17.8642, -16.6991, -16.5568, -16.3714, -16.9949,\n        -17.1926, -16.9671, -16.8230, -15.7120, -16.2652, -17.3596, -16.8273,\n        -16.4941, -14.3067, -15.1746, -16.7538, -16.6177, -14.5924, -17.4868,\n        -16.9961, -15.2027, -16.1179, -17.8136, -16.9221, -17.4402, -17.1182,\n        -17.0236, -16.9496, -16.7210, -17.2224, -16.8340, -14.6614, -17.7063,\n        -17.6374, -17.6177, -17.2496, -16.2857, -16.8705, -16.0419, -16.8412,\n        -17.6452, -13.3405, -17.0936, -17.2620, -14.8293, -16.4840, -17.7907,\n        -17.6367, -16.9794, -17.4806, -16.8857, -17.2911, -16.7659, -15.7899,\n        -16.0170, -14.3231, -16.3894, -16.0958, -13.7606, -15.9562, -16.4767,\n        -17.6906, -17.0464, -15.9133, -16.8948, -16.1856, -15.6871, -17.4597,\n        -15.1920, -17.2234, -14.9576, -17.7829, -17.8947, -16.1404, -17.2103,\n        -16.7290, -15.5131, -17.3531, -15.0447, -17.8739, -15.7056, -16.7523,\n        -15.4276, -16.6752, -16.6374, -17.0997, -17.2698, -16.1508, -16.9022,\n        -15.6754, -17.2008, -17.5945, -16.4966, -17.2604, -15.9539, -17.1234,\n        -17.6056, -17.3484, -16.1133, -17.3953, -16.8830, -16.6132, -17.1589,\n        -17.1432, -17.6974, -17.3527, -16.8420, -16.8880,  -7.7879, -16.9901,\n        -15.8571, -15.3417, -16.7466, -14.3681, -13.3931, -16.2099, -15.7932,\n        -16.7962, -17.4833, -13.5962, -17.7920, -15.0972, -16.2645, -15.1858,\n        -16.6797, -16.3931, -17.5987, -16.3806, -16.6691, -17.5670, -13.5660,\n        -16.1329, -16.6498, -17.7633, -11.9445, -17.6930, -16.2073, -17.2636,\n        -17.8689, -15.8726, -16.3609, -16.1388, -16.8463, -16.7943, -16.9481,\n        -17.1803, -13.8762, -16.2069, -16.5714, -13.3702, -16.9707, -15.1451,\n        -17.7622, -10.2545, -16.9429, -12.9870, -16.5277, -16.6480, -15.0934,\n        -17.5117, -17.1645, -17.1222, -17.2802, -15.8978, -15.6626, -17.8973,\n        -15.4456, -14.8156, -16.5574, -16.9696, -15.0446, -14.8778, -16.0970,\n        -17.7404, -16.6000, -16.7870, -15.6458, -14.8563, -15.2952, -16.9022,\n        -17.1012, -17.8283, -16.9903, -16.9610, -11.8376, -16.1911, -17.2052,\n        -17.0333, -16.5970, -16.3592, -16.3686, -17.7152, -16.5726, -16.6921,\n        -16.8786, -17.3966, -16.6916, -16.8164, -16.4075, -16.7753, -16.3588,\n        -15.2915, -17.0675, -17.2979, -15.7603, -17.8791, -15.6601, -16.5525,\n        -16.0544, -16.3048, -14.6642, -14.9261])\n\n\nC:\\Users\\UOS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning:\n\nImplicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n\n\n\n\n\n\nhttps://openreview.net/forum?id=BJJLHbb0-"
  },
  {
    "objectID": "posts/DAGMM/2023-10-15.html#deep-autoencoding-gaussian-mixture-model-for-arrhythmia-dataset",
    "href": "posts/DAGMM/2023-10-15.html#deep-autoencoding-gaussian-mixture-model-for-arrhythmia-dataset",
    "title": "[DAGMM] DAGMM: for arrhythmia data set",
    "section": "",
    "text": "### imports\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport sys\n\n\n### data 파일\nfile_path = 'C:\\\\Users\\\\UOS\\\\Desktop\\\\연구\\\\5. 데이터\\\\data\\\\arrhythmia\\\\arrhythmia.data'\n\ndf = pd.read_csv(file_path, header=None)\ndf = df.replace('?', 0)\ndf = df.astype('float64')\n\ndata_array = df.values\ndata_array = torch.autograd.Variable(torch.from_numpy(data_array).float())\ndata_array.shape\n\ntorch.Size([452, 280])\n\n\n\nparser = argparse.ArgumentParser(description='parser for argparse test')\n\nparser.add_argument('--input_dim', type=int, default=data_array.shape[-1])\nparser.add_argument('--enc_hidden_dim', type=str, default='10,2')\nparser.add_argument('--dec_hidden_dim', type=str, default='10')\nparser.add_argument('--est_hidden_dim', type=str, default='4, 10, 2')\nparser.add_argument('--dropout', action='store_true', default=0.5)\nparser.add_argument('--learning_rate', type=float, default=0.001)\nparser.add_argument('--num_epoch', type=int, default=10)\n\nif 'ipykernel_launcher' in sys.argv[0]:\n    sys.argv = [sys.argv[0]]  \n\nargs = parser.parse_args()\n\nenc_hidden_dim = args.enc_hidden_dim.split(',')\ndec_hidden_dim = args.dec_hidden_dim.split(',')\nest_hidden_dim = args.est_hidden_dim.split(',')\n\nargs.enc_hidden_dim_list = []\nargs.dec_hidden_dim_list = []\nargs.est_hidden_dim_list = []\n\nargs.enc_hidden_dim_list.append(args.input_dim)\n\nfor i in enc_hidden_dim:\n    args.enc_hidden_dim_list.append(int(i))\n\nargs.enc_hidden_dim_list\n\nargs.dec_hidden_dim_list.append(args.enc_hidden_dim_list[-1])\n\nfor i in dec_hidden_dim:\n    args.dec_hidden_dim_list.append(int(i))\n\nargs.dec_hidden_dim_list.append(args.input_dim)\n\nargs.dec_hidden_dim_list\n\nfor i in est_hidden_dim:\n    args.est_hidden_dim_list.append(int(i))\n\nargs.est_hidden_dim_list\n\nargs\n\nNamespace(input_dim=280, enc_hidden_dim='10,2', dec_hidden_dim='10', est_hidden_dim='4, 10, 2', dropout=0.5, learning_rate=0.001, num_epoch=10, enc_hidden_dim_list=[280, 10, 2], dec_hidden_dim_list=[2, 10, 280], est_hidden_dim_list=[4, 10, 2])\n\n\n\n### compresssion network\nclass midlayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(midlayer, self).__init__()\n        self.fc_layer   = nn.Linear(input_dim, hidden_dim)\n        self.activation = nn.Tanh()\n    \n    def forward(self, input):\n        out = self.fc_layer(input)        \n        out = self.activation(out)\n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Encoder, self).__init__()\n        \n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Linear(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n\n    def forward(self, input):\n        out = self.layer(input)\n        return out\n    \nclass Decoder(nn.Module):\n    def __init__(self, hidden_dim_list):\n        super(Decoder, self).__init__()\n\n        layer_list = []\n        for i in range(len(hidden_dim_list)-2):\n            layer_list.append(midlayer(hidden_dim_list[i], hidden_dim_list[i+1]))\n        \n        layer_list.append(midlayer(hidden_dim_list[i+1], hidden_dim_list[i+2]))\n        self.layer = nn.Sequential(*layer_list)\n    \n    def forward(self, input):\n        out = self.layer(input)\n        return out\n\nclass CompressionNet(nn.Module):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list):\n        super().__init__()\n        self.encoder = Encoder(enc_hidden_dim_list)\n        self.decoder = Decoder(dec_hidden_dim_list)\n\n        self._reconstruction_loss = nn.MSELoss()\n\n    def forward(self, input):\n        out = self.encoder(input)\n        out = self.decoder(out)\n        return out\n\n    def encode(self, input):\n        return self.encoder(input)\n\n    def decode(self, input):\n        return self.decoder(input)\n\n    def reconstuction_loss(self, input, input_target):\n        target_hat = self(input)\n        return self._reconstruction_loss(target_hat, input_target)\n\n\n### reconstructed error\neps = torch.autograd.Variable(torch.FloatTensor([1.e-8]), requires_grad=False)\n\ndef relative_euclidean_distance(x1, x2, eps=eps):\n    num = torch.norm(x1 - x2, p=2, dim=1)\n    denom = torch.norm(x1, p=2, dim=1)\n    return num / torch.max(denom, eps)\n\ndef cosine_similarity(x1, x2, eps=eps):\n    dot_prod = torch.sum(x1 * x2, dim=1)\n    dist_x1 = torch.norm(x1, p=2, dim=1)\n    dist_x2 = torch.norm(x2, p=2, dim=1)\n    return dot_prod / torch.max(dist_x1*dist_x2, eps)\n\n\n### estimation network\nclass Estimation(nn.Module):\n    def __init__(self, est_hidden_dim_list):\n        super().__init__()\n        \n        layer_list = []\n        for i in range(len(est_hidden_dim_list)-2):\n            layer_list.append(midlayer(est_hidden_dim_list[i], est_hidden_dim_list[i+1]))\n        \n        layer_list.append(nn.Dropout(p=0.5))\n        layer_list.append(nn.Linear(est_hidden_dim_list[-2], est_hidden_dim_list[-1]))\n        layer_list.append(nn.Softmax())\n        self.net = nn.Sequential(*layer_list)\n        \n    def forward(self, input):\n        out = self.net(input)\n        return out\n\n\n### Mixture\nclass Mixture(nn.Module):\n    def __init__(self, latent_dimension):\n        super().__init__()\n        self.latent_dimension = latent_dimension\n\n        self.Phi    = np.random.random([1])\n        self.Phi    = torch.from_numpy(self.Phi).float()\n        self.Phi    = nn.Parameter(self.Phi, requires_grad = False)\n\n        self.mu     = 2.*np.random.random([latent_dimension]) - 0.5\n        self.mu     = torch.from_numpy(self.mu).float()\n        self.mu     = nn.Parameter(self.mu, requires_grad = False)\n\n        self.Sigma  = np.eye(latent_dimension, latent_dimension)\n        self.Sigma  = torch.from_numpy(self.Sigma).float()\n        self.Sigma  = nn.Parameter(self.Sigma, requires_grad = False)\n        \n        self.eps_Sigma  = torch.FloatTensor(np.diag([1.e-8 for _ in range(latent_dimension)]))\n\n    def forward(self, est_inputs, with_log = True):\n        batch_size, _   = est_inputs.shape\n        out_values  = []\n        inv_sigma   = torch.inverse(self.Sigma)\n        det_sigma   = np.linalg.det(self.Sigma.data.cpu().numpy())\n        det_sigma   = torch.from_numpy(det_sigma.reshape([1])).float()\n        det_sigma   = torch.autograd.Variable(det_sigma)\n        for est_input in est_inputs:\n            diff    = (est_input - self.mu).view(-1,1)\n            out     = -0.5 * torch.mm(torch.mm(diff.view(1,-1), inv_sigma), diff)\n            out     = (self.Phi * torch.exp(out)) / torch.sqrt(2. * np.pi * det_sigma)\n            if with_log:\n                out = -torch.log(out)\n            out_values.append(float(out.data.cpu().numpy()))\n\n        out = torch.autograd.Variable(torch.FloatTensor(out_values))\n        return out\n    \n    def _update_parameters(self, samples, affiliations):\n        if not self.training:\n            return\n\n        batch_size, _ = samples.shape\n\n        # Updating phi.\n        phi = torch.mean(affiliations)\n        self.Phi.data = phi.data\n\n        # Updating mu.\n        num = 0.\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            num     += gamma_i * z_i\n        \n        denom        = torch.sum(affiliations)\n        self.mu.data = (num / denom).data\n\n        # Updating Sigma.\n        mu  = self.mu\n        num = None\n        for i in range(batch_size):\n            z_i     = samples[i, :]\n            gamma_i = affiliations[i]\n            diff    = (z_i - mu).view(-1, 1)\n            to_add  = gamma_i * torch.mm(diff, diff.view(1, -1))\n            if num is None:\n                num = to_add\n            else:\n                num += to_add\n\n        denom           = torch.sum(affiliations)\n        self.Sigma.data = (num / denom).data + self.eps_Sigma\n\n\nclass GMM(nn.Module):\n    def __init__(self, num_mixtures, latent_dimension):\n        super().__init__()\n        self.num_mixtures       = num_mixtures\n        self.latent_dimension   = latent_dimension\n\n        mixtures        = [Mixture(latent_dimension) for _ in range(num_mixtures)]\n        self.mixtures   = nn.ModuleList(mixtures)\n    \n    def forward(self, est_inputs):\n        out = None\n        for mixture in self.mixtures:\n            to_add  = mixture(est_inputs, with_log = False)\n            if out is None:\n                out = to_add\n            else:\n                out += to_add\n        return -torch.log(out)\n    \n    def _update_mixtures_parameters(self, samples, mixtures_affiliations):\n        if not self.training:\n            return\n\n        for i, mixture in enumerate(self.mixtures):\n            affiliations = mixtures_affiliations[:, i]\n            mixture._update_parameters(samples, affiliations)\n\n\n### model\nclass DAGMM(nn.Module):\n    def __init__(self, compression_module, estimation_module, gmm_module):\n        super().__init__()\n\n        self.compressor = compression_module\n        self.estimator  = estimation_module\n        self.gmm        = gmm_module\n\n    def forward(self, input):\n        encoded = self.compressor.encode(input)\n        decoded = self.compressor.decode(encoded)\n\n        relative_ed     = relative_euclidean_distance(input, decoded)\n        cosine_sim      = cosine_similarity(input, decoded)\n\n        relative_ed     = relative_ed.view(-1, 1)\n        cosine_sim      = relative_ed.view(-1, 1)\n        latent_vectors  = torch.cat([encoded, relative_ed, cosine_sim], dim=1)\n\n        if self.training:\n            mixtures_affiliations = self.estimator(latent_vectors)\n            self.gmm._update_mixtures_parameters(latent_vectors,\n                                                 mixtures_affiliations)\n        return self.gmm(latent_vectors)\n\n\nclass DAGMMArrhythmia(DAGMM):\n    def __init__(self, enc_hidden_dim_list, dec_hidden_dim_list, est_hidden_dim_list):\n        compressor  = CompressionNet(enc_hidden_dim_list, dec_hidden_dim_list)\n        estimator   = Estimation(est_hidden_dim_list)\n        gmm = GMM(num_mixtures=2, latent_dimension=4)\n\n        super().__init__(compression_module = compressor,\n                         estimation_module  = estimator,\n                         gmm_module         = gmm)\n\n\n### tests\ndef test_dagmm():\n    net = DAGMMArrhythmia(args.enc_hidden_dim_list, args.dec_hidden_dim_list, args.est_hidden_dim_list)\n    out = net(data_array)\n    print(out)\n\ndef convert_to_var(input):\n    out = torch.from_numpy(input).float()\n    out = torch.autograd.Variable(out)\n    return out\n\ndef test_update_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n    mix              = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    affiliations     = np.random.random([batch_size])\n    latent_vectors   = convert_to_var(latent_vectors)\n    affiliations     = convert_to_var(affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n    mix.train()\n    mix._update_parameters(latent_vectors, affiliations)\n\n    for param in mix.parameters():\n        print(param)\n\n\ndef test_forward_mixture():\n    batch_size       = 5\n    latent_dimension = 7\n\n    mix = Mixture(latent_dimension)\n    latent_vectors   = np.random.random([batch_size, latent_dimension])\n    latent_vectors   = convert_to_var(latent_vectors)\n\n    mix.train()\n    out = mix(latent_vectors)\n    print(out)\n\n\ndef test_update_gmm():\n    batch_size      = int(5)\n    latent_dimension= 7\n    num_mixtures    = 2\n\n    gmm = GMM(num_mixtures, latent_dimension)\n\n    latent_vectors  = np.random.random([batch_size, latent_dimension])\n    latent_vectors  = convert_to_var(latent_vectors)\n\n    affiliations    = np.random.random([batch_size, num_mixtures])\n    affiliations    = convert_to_var(affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n    gmm.train()\n    gmm._update_mixtures_parameters(latent_vectors, affiliations)\n\n    for param in gmm.parameters():\n        print(param)\n\n\nif __name__ == '__main__':\n    test_update_mixture()\n    test_forward_mixture()\n    test_update_gmm()\n    test_dagmm()\n\nParameter containing:\ntensor([0.1173])\nParameter containing:\ntensor([ 1.1475, -0.3912, -0.4815,  0.8666,  1.2454, -0.2445,  1.4350])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.4673)\nParameter containing:\ntensor([0.5283, 0.2903, 0.5008, 0.5089, 0.3621, 0.4749, 0.4967])\nParameter containing:\ntensor([[ 0.0439,  0.0255, -0.0151,  0.0544,  0.0647,  0.0090,  0.0726],\n        [ 0.0255,  0.0419, -0.0004,  0.0360,  0.0314,  0.0235,  0.0499],\n        [-0.0151, -0.0004,  0.0240,  0.0005, -0.0159,  0.0520, -0.0083],\n        [ 0.0544,  0.0360,  0.0005,  0.0954,  0.0900,  0.0640,  0.1063],\n        [ 0.0647,  0.0314, -0.0159,  0.0900,  0.1013,  0.0334,  0.1124],\n        [ 0.0090,  0.0235,  0.0520,  0.0640,  0.0334,  0.1681,  0.0645],\n        [ 0.0726,  0.0499, -0.0083,  0.1063,  0.1124,  0.0645,  0.1351]])\ntensor([4.8554, 4.6761, 4.4661, 4.8710, 4.8982])\nParameter containing:\ntensor([0.9724])\nParameter containing:\ntensor([ 0.3688,  0.9626,  0.7468, -0.3848,  0.9530,  0.7081,  0.9869])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor([0.3145])\nParameter containing:\ntensor([1.4138, 0.4490, 1.3144, 0.2030, 1.1138, 0.8565, 0.0193])\nParameter containing:\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])\nParameter containing:\ntensor(0.6101)\nParameter containing:\ntensor([0.2734, 0.7025, 0.5866, 0.5380, 0.4088, 0.3543, 0.5669])\nParameter containing:\ntensor([[ 0.0827, -0.0232, -0.0216, -0.0052, -0.0009,  0.0318, -0.0217],\n        [-0.0232,  0.0179,  0.0211,  0.0308,  0.0019, -0.0294,  0.0174],\n        [-0.0216,  0.0211,  0.0355,  0.0366, -0.0125, -0.0311,  0.0099],\n        [-0.0052,  0.0308,  0.0366,  0.0781,  0.0157, -0.0543,  0.0373],\n        [-0.0009,  0.0019, -0.0125,  0.0157,  0.0748,  0.0095,  0.0413],\n        [ 0.0318, -0.0294, -0.0311, -0.0543,  0.0095,  0.0582, -0.0249],\n        [-0.0217,  0.0174,  0.0099,  0.0373,  0.0413, -0.0249,  0.0391]])\nParameter containing:\ntensor(0.7920)\nParameter containing:\ntensor([0.3664, 0.7017, 0.5854, 0.6048, 0.4577, 0.3511, 0.5929])\nParameter containing:\ntensor([[ 0.1143, -0.0355, -0.0402, -0.0143,  0.0052,  0.0471, -0.0279],\n        [-0.0355,  0.0251,  0.0335,  0.0389, -0.0113, -0.0418,  0.0165],\n        [-0.0402,  0.0335,  0.0528,  0.0540, -0.0270, -0.0540,  0.0137],\n        [-0.0143,  0.0389,  0.0540,  0.0882, -0.0128, -0.0710,  0.0290],\n        [ 0.0052, -0.0113, -0.0270, -0.0128,  0.0901,  0.0411,  0.0349],\n        [ 0.0471, -0.0418, -0.0540, -0.0710,  0.0411,  0.0829, -0.0191],\n        [-0.0279,  0.0165,  0.0137,  0.0290,  0.0349, -0.0191,  0.0330]])\ntensor([-16.9612, -17.8355, -17.1198, -17.8214, -16.9765, -13.4832, -17.8883,\n        -16.8427, -16.7266, -17.4553, -16.4203, -16.9409, -17.6146, -15.1495,\n        -15.9379, -14.9479, -16.1854, -17.5789, -16.6280, -16.3914, -17.8705,\n        -16.4540, -17.6894, -17.6952, -17.9105, -16.9977, -16.9534, -17.4336,\n        -17.0536, -16.7684, -16.9417, -16.7795, -17.7103, -16.8721, -16.0556,\n        -16.6950, -17.5597, -17.4694, -17.2260, -16.6612, -17.3744, -16.9852,\n        -15.9520, -15.9058, -16.7894, -16.7476, -16.5294, -17.4851, -17.1710,\n        -17.6680, -17.7744, -17.4803, -16.1885, -14.6914, -17.6060, -17.8666,\n        -15.9840, -16.8587, -17.0594, -15.2725, -12.6417, -16.6181, -16.4920,\n        -16.8071, -17.3286, -17.1299, -17.0695, -15.1893, -16.3951, -16.9207,\n        -17.8430, -17.5642, -17.4121, -17.1715, -16.3110, -16.8747,  -9.4447,\n        -17.1424, -16.8413, -15.5074, -16.9420, -16.5711, -16.9047, -17.0329,\n        -16.1576, -12.1645, -15.3441, -17.2765, -13.0270, -17.2411, -16.8029,\n        -17.0612, -16.9580, -16.4213, -16.3675, -17.3942, -16.8937, -16.7398,\n        -16.9648, -14.1587, -17.4364, -15.2745, -16.3091, -16.2499, -16.8516,\n        -17.1280, -13.5346, -17.6581, -13.9843, -17.1864, -16.6246, -16.5804,\n        -16.8307, -15.3374, -17.8115, -17.0566, -16.4458, -15.4481, -17.3106,\n        -16.7912, -17.0978, -16.9524, -16.8946, -16.9953, -16.8141, -16.8253,\n        -17.8658, -15.8182, -17.7361, -17.1825, -17.2128, -17.8637, -15.8899,\n        -13.8260, -17.1471, -16.3404, -17.5933, -16.9136, -17.3709, -17.0664,\n        -16.5006, -12.6482, -16.4738, -16.5775, -16.7379, -17.5284, -16.8920,\n        -17.3995, -17.0317, -17.3837, -17.0141, -15.2970, -16.1848, -16.9914,\n        -17.8913, -17.4832, -17.8995, -16.2905, -15.0831, -16.9196, -17.1523,\n        -16.7555, -17.1981, -15.8082, -16.2761, -17.2934, -16.5064, -16.0278,\n        -17.3910, -17.2586, -15.3088, -16.0010, -17.4784, -17.1952, -16.1224,\n        -17.5613, -16.2742, -17.6207, -16.7494, -16.2702, -17.0538, -16.6950,\n        -17.7900, -17.1539, -15.8621, -16.9515, -16.4764, -16.7403, -15.7678,\n        -15.0590, -17.3892, -16.1987, -14.6992, -16.0648, -17.3860, -17.0080,\n        -17.8890, -16.5353, -15.8924, -16.6831, -17.8159, -17.6439, -16.6141,\n        -17.5179, -11.3079, -14.3721, -16.9241, -11.8753, -12.8442, -17.4653,\n        -17.2166, -16.8058, -17.2556, -15.4820, -17.3901, -16.7357, -16.1637,\n        -12.8449, -14.0127, -17.8357, -17.0824, -16.0741, -16.9676, -17.7164,\n        -16.6101, -15.9101, -16.6739, -16.7744, -15.0311, -16.1429, -17.6085,\n        -17.2562, -17.5409, -16.1831, -15.0784, -15.8370, -17.1719, -16.1404,\n        -17.5857, -17.6243, -17.8642, -16.6991, -16.5568, -16.3714, -16.9949,\n        -17.1926, -16.9671, -16.8230, -15.7120, -16.2652, -17.3596, -16.8273,\n        -16.4941, -14.3067, -15.1746, -16.7538, -16.6177, -14.5924, -17.4868,\n        -16.9961, -15.2027, -16.1179, -17.8136, -16.9221, -17.4402, -17.1182,\n        -17.0236, -16.9496, -16.7210, -17.2224, -16.8340, -14.6614, -17.7063,\n        -17.6374, -17.6177, -17.2496, -16.2857, -16.8705, -16.0419, -16.8412,\n        -17.6452, -13.3405, -17.0936, -17.2620, -14.8293, -16.4840, -17.7907,\n        -17.6367, -16.9794, -17.4806, -16.8857, -17.2911, -16.7659, -15.7899,\n        -16.0170, -14.3231, -16.3894, -16.0958, -13.7606, -15.9562, -16.4767,\n        -17.6906, -17.0464, -15.9133, -16.8948, -16.1856, -15.6871, -17.4597,\n        -15.1920, -17.2234, -14.9576, -17.7829, -17.8947, -16.1404, -17.2103,\n        -16.7290, -15.5131, -17.3531, -15.0447, -17.8739, -15.7056, -16.7523,\n        -15.4276, -16.6752, -16.6374, -17.0997, -17.2698, -16.1508, -16.9022,\n        -15.6754, -17.2008, -17.5945, -16.4966, -17.2604, -15.9539, -17.1234,\n        -17.6056, -17.3484, -16.1133, -17.3953, -16.8830, -16.6132, -17.1589,\n        -17.1432, -17.6974, -17.3527, -16.8420, -16.8880,  -7.7879, -16.9901,\n        -15.8571, -15.3417, -16.7466, -14.3681, -13.3931, -16.2099, -15.7932,\n        -16.7962, -17.4833, -13.5962, -17.7920, -15.0972, -16.2645, -15.1858,\n        -16.6797, -16.3931, -17.5987, -16.3806, -16.6691, -17.5670, -13.5660,\n        -16.1329, -16.6498, -17.7633, -11.9445, -17.6930, -16.2073, -17.2636,\n        -17.8689, -15.8726, -16.3609, -16.1388, -16.8463, -16.7943, -16.9481,\n        -17.1803, -13.8762, -16.2069, -16.5714, -13.3702, -16.9707, -15.1451,\n        -17.7622, -10.2545, -16.9429, -12.9870, -16.5277, -16.6480, -15.0934,\n        -17.5117, -17.1645, -17.1222, -17.2802, -15.8978, -15.6626, -17.8973,\n        -15.4456, -14.8156, -16.5574, -16.9696, -15.0446, -14.8778, -16.0970,\n        -17.7404, -16.6000, -16.7870, -15.6458, -14.8563, -15.2952, -16.9022,\n        -17.1012, -17.8283, -16.9903, -16.9610, -11.8376, -16.1911, -17.2052,\n        -17.0333, -16.5970, -16.3592, -16.3686, -17.7152, -16.5726, -16.6921,\n        -16.8786, -17.3966, -16.6916, -16.8164, -16.4075, -16.7753, -16.3588,\n        -15.2915, -17.0675, -17.2979, -15.7603, -17.8791, -15.6601, -16.5525,\n        -16.0544, -16.3048, -14.6642, -14.9261])\n\n\nC:\\Users\\UOS\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning:\n\nImplicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n\n\n\n\n\n\nhttps://openreview.net/forum?id=BJJLHbb0-"
  }
]